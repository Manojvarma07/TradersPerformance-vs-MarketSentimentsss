{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b9684e7-df8b-4493-9dc2-f34ecb400c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sentiment rows = 2644 , trades rows = 211224\n",
      "\n",
      "-- Sentiment (raw) --\n",
      "shape: (2644, 4)\n",
      "columns: ['timestamp', 'value', 'classification', 'date']\n",
      "missing (top 10):\n",
      "timestamp         0\n",
      "value             0\n",
      "classification    0\n",
      "date              0\n",
      "duplicate rows: 0\n",
      "    timestamp  value classification        date\n",
      "0  1517463000     30           Fear  2018-02-01\n",
      "1  1517549400     15   Extreme Fear  2018-02-02\n",
      "2  1517635800     40           Fear  2018-02-03\n",
      "\n",
      "-- Trades (raw) --\n",
      "shape: (211224, 16)\n",
      "columns: ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp']\n",
      "missing (top 10):\n",
      "Account            0\n",
      "Coin               0\n",
      "Execution Price    0\n",
      "Size Tokens        0\n",
      "Size USD           0\n",
      "Side               0\n",
      "Timestamp IST      0\n",
      "Start Position     0\n",
      "Direction          0\n",
      "Closed PnL         0\n",
      "duplicate rows: 0\n",
      "                                      Account  Coin  Execution Price  Size Tokens  Size USD Side     Timestamp IST  Start Position Direction  Closed PnL                                                    Transaction Hash     Order ID  Crossed       Fee      Trade ID     Timestamp\n",
      "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9769       986.87   7872.16  BUY  02-12-2024 22:50        0.000000       Buy         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac0f37caef8a734502ec49  52017706630     True  0.345404  8.950000e+14  1.730000e+12\n",
      "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9800        16.00    127.68  BUY  02-12-2024 22:50      986.524596       Buy         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac0f37caef8a734502ec49  52017706630     True  0.005600  4.430000e+14  1.730000e+12\n",
      "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9855       144.09   1150.63  BUY  02-12-2024 22:50     1002.518996       Buy         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac0f37caef8a734502ec49  52017706630     True  0.050431  6.600000e+14  1.730000e+12\n",
      "\n",
      "Normalized sentiment cols: ['timestamp', 'value', 'classification', 'date']\n",
      "Normalized trades cols: ['account', 'coin', 'execution_price', 'size_tokens', 'size_usd', 'side', 'timestamp_ist', 'start_position', 'direction', 'closed_pnl', 'transaction_hash', 'order_id', 'crossed', 'fee', 'trade_id', 'timestamp']\n",
      "\n",
      "Sentiment normalized: rows = 2644 | date range: 2018-02-01 -> 2025-05-02\n",
      "\n",
      "Detected trade columns:\n",
      "  account: account\n",
      "  timestamp: timestamp\n",
      "  closed_pnl: closed_pnl\n",
      "  size_usd: size_usd\n",
      "  size_tokens: size_tokens\n",
      "  any_size: size_usd\n",
      "  side: side\n",
      "  leverage: start_position\n",
      "\n",
      "Timestamp parsing method: epoch_unit_ms\n",
      "Parsed non-null: 211224 of 211224\n",
      "Sample datetimes: [Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20')]\n",
      "Range: 2023-03-28 10:40:00 -> 2025-06-15 15:06:40\n",
      "Dropped 0 rows (invalid datetime). Remaining: 211224\n",
      "Trades date range: 2023-03-28 -> 2025-06-15\n",
      "\n",
      "Canonical columns present: ['account', 'closed_pnl', 'size_usd', 'size_tokens', 'datetime', 'date', 'side', 'leverage']\n",
      "\n",
      "--- Part A Summary ---\n",
      "Trades:          rows,cols: (211224, 20)\n",
      "Daily_account:   rows,cols: (102, 12)\n",
      "Daily (platform):rows,cols: (7, 8)\n",
      "\n",
      "Missing values (trades) top 10:\n",
      "account      0\n",
      "coin         0\n",
      "is_win       0\n",
      "date         0\n",
      "datetime     0\n",
      "timestamp    0\n",
      "trade_id     0\n",
      "fee          0\n",
      "crossed      0\n",
      "order_id     0\n",
      "\n",
      "Missing values (daily_account) top 10:\n",
      "date              0\n",
      "account           0\n",
      "daily_pnl         0\n",
      "trades_count      0\n",
      "win_count         0\n",
      "avg_trade_size    0\n",
      "avg_leverage      0\n",
      "long_count        0\n",
      "short_count       0\n",
      "win_rate          0\n",
      "\n",
      "Duplicates: trades = 0\n",
      "Duplicates: daily_account = 0\n",
      "\n",
      "Leverage summary (trades):\n",
      "count    2.112240e+05\n",
      "mean    -2.994625e+04\n",
      "std      6.738074e+05\n",
      "min     -1.433463e+07\n",
      "25%     -3.762311e+02\n",
      "50%      8.472793e+01\n",
      "75%      9.337278e+03\n",
      "max      3.050948e+07\n",
      "\n",
      "Top 5 accounts by total_pnl:\n",
      "                                   account    total_pnl  total_trades  avg_win_rate\n",
      "0xb1231a4a2dd02f2276fa3c5e2a2f3436e6bfed23 2.143383e+06         14733      0.306417\n",
      "0x083384f897ee0f19899168e3b1bec365f52a9012 1.600230e+06          3818      0.215758\n",
      "0xbaaaf6571ab7d571043ff1e313a9609a10637864 9.401638e+05         21192      0.733766\n",
      "0x513b8629fe877bb581bf244e326a047b249c4ff1 8.404226e+05         12236      0.272598\n",
      "0xbee1707d6b44d4d52bfe19e41f8a828645437aab 8.360806e+05         40184      0.439046\n",
      "\n",
      "Saved to outputs/: daily_account_metrics.csv, daily_metrics.csv, trades_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART A (FIXED): Data Preparation\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SENTIMENT_PATH = \"fear_greed_index.csv\"\n",
    "TRADER_PATH    = \"historical_data.csv\"\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "#  Load files \n",
    "if not os.path.exists(SENTIMENT_PATH):\n",
    "    raise FileNotFoundError(f\"Sentiment file not found at: {SENTIMENT_PATH}\")\n",
    "if not os.path.exists(TRADER_PATH):\n",
    "    raise FileNotFoundError(f\"Trades file not found at: {TRADER_PATH}\")\n",
    "\n",
    "sent   = pd.read_csv(SENTIMENT_PATH)\n",
    "trades = pd.read_csv(TRADER_PATH, low_memory=False)\n",
    "\n",
    "print(\"Loaded: sentiment rows =\", len(sent), \", trades rows =\", len(trades))\n",
    "\n",
    "# Document shapes\n",
    "def doc_df(df, name, n_show=3):\n",
    "    print(f\"\\n-- {name} --\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"columns:\", df.columns.tolist())\n",
    "    print(\"missing (top 10):\")\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(10).to_string())\n",
    "    print(\"duplicate rows:\", df.duplicated().sum())\n",
    "    print(df.head(n_show).to_string())   # FIX: replaced display() with print()\n",
    "\n",
    "doc_df(sent,   \"Sentiment (raw)\")\n",
    "doc_df(trades, \"Trades (raw)\")\n",
    "\n",
    "#  Normalize column names \n",
    "sent.columns   = sent.columns.str.strip().str.lower().str.replace(r'\\s+', '_', regex=True)\n",
    "trades.columns = (trades.columns\n",
    "                  .str.strip()\n",
    "                  .str.replace('\\xa0', ' ', regex=False)\n",
    "                  .str.strip()\n",
    "                  .str.replace(r'\\s+', '_', regex=True)\n",
    "                  .str.lower())\n",
    "\n",
    "print(\"\\nNormalized sentiment cols:\", list(sent.columns))\n",
    "print(\"Normalized trades cols:\",    list(trades.columns))\n",
    "\n",
    "#  Detect sentiment columns \n",
    "sent_date_col  = next((c for c in sent.columns if \"date\"  in c), None)\n",
    "sent_label_col = next((c for c in sent.columns\n",
    "                       if any(k in c for k in [\"class\",\"sent\",\"classification\"])), None)\n",
    "if sent_date_col is None or sent_label_col is None:\n",
    "    raise ValueError(\"Couldn't detect sentiment date/class columns. Found: \" + \", \".join(sent.columns))\n",
    "\n",
    "sent['date']      = pd.to_datetime(sent[sent_date_col], errors='coerce').dt.date\n",
    "sent['sentiment'] = sent[sent_label_col].astype(str).str.strip().str.capitalize()\n",
    "sent = sent[['date', 'sentiment']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nSentiment normalized: rows =\", len(sent),\n",
    "      \"| date range:\", sent['date'].min(), \"->\", sent['date'].max())\n",
    "\n",
    "#  Detect trade columns \n",
    "def pick(cols):\n",
    "    for c in cols:\n",
    "        if c in trades.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "acct_col        = pick(['account','acct','user','client'])\n",
    "timestamp_col   = pick(['timestamp','timestamp_ist','timestamp_utc','time','ts','datetime'])\n",
    "closed_pnl_col  = next((c for c in trades.columns if 'closed' in c and 'pnl' in c), None) \\\n",
    "                  or pick(['closed_pnl','realized_pnl','pnl','profit'])\n",
    "size_usd_col    = pick(['size_usd','sizeusd','size_usd.'])\n",
    "size_tokens_col = pick(['size_tokens','size_token'])\n",
    "size_col_any    = size_usd_col or size_tokens_col or pick(['size','qty','quantity'])\n",
    "side_col        = pick(['side','direction','trade_side'])\n",
    "leverage_col    = pick(['leverage','lev','leverage_ratio','start_position','margin'])\n",
    "\n",
    "print(\"\\nDetected trade columns:\")\n",
    "for k, v in [(\"account\", acct_col), (\"timestamp\", timestamp_col),\n",
    "              (\"closed_pnl\", closed_pnl_col), (\"size_usd\", size_usd_col),\n",
    "              (\"size_tokens\", size_tokens_col), (\"any_size\", size_col_any),\n",
    "              (\"side\", side_col), (\"leverage\", leverage_col)]:\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "required_missing = [r for r in [acct_col, timestamp_col, closed_pnl_col] if r is None]\n",
    "if required_missing:\n",
    "    raise ValueError(f\"Missing required columns: {required_missing}. Available: {list(trades.columns)}\")\n",
    "\n",
    "#  Convert numeric fields safely \n",
    "trades[closed_pnl_col] = pd.to_numeric(trades[closed_pnl_col], errors='coerce')\n",
    "if size_col_any:\n",
    "    trades[size_col_any] = pd.to_numeric(trades[size_col_any], errors='coerce')\n",
    "if leverage_col and leverage_col in trades.columns:\n",
    "    trades[leverage_col] = pd.to_numeric(trades[leverage_col], errors='coerce')\n",
    "\n",
    "#  Robust timestamp parsing \n",
    "def try_parse_epoch_unit(series):\n",
    "    \"\"\"Try units ns/us/ms/s; return (parsed_series, unit) or (None, None).\"\"\"\n",
    "    snum = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if len(snum) == 0:\n",
    "        return None, None\n",
    "    sample = snum.sample(min(len(snum), 200), random_state=1)\n",
    "    best_unit, best_score = None, -1\n",
    "    for u in ['ns', 'us', 'ms', 's']:\n",
    "        try:\n",
    "            #  cast to plain int64 numpy array to avoid overflow with Int64 nullable\n",
    "            dt_sample = pd.to_datetime(sample.values.astype('int64'), unit=u, errors='coerce')\n",
    "            score = pd.Series(dt_sample).dt.year.between(2009, 2035).sum()\n",
    "            if score > best_score:\n",
    "                best_unit, best_score = u, score\n",
    "        except Exception:\n",
    "            continue\n",
    "    if best_score <= 0:\n",
    "        return None, None\n",
    "    try:\n",
    "        parsed = pd.to_datetime(\n",
    "            pd.to_numeric(series, errors='coerce').values.astype('float64').astype('int64'),\n",
    "            unit=best_unit, errors='coerce'\n",
    "        )\n",
    "        return pd.Series(parsed, index=series.index), best_unit\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def clean_timestamp_strings(s):\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r'\\s+[A-Z]{2,4}$', '', regex=True)  # strip trailing TZ abbreviations\n",
    "    s = s.str.replace(',', ' ', regex=False)\n",
    "    return s\n",
    "\n",
    "# Primary: numeric epoch detection\n",
    "parsed_dt, unit_used = try_parse_epoch_unit(trades[timestamp_col])\n",
    "if parsed_dt is None or parsed_dt.isna().all():\n",
    "    cleaned   = clean_timestamp_strings(trades[timestamp_col])\n",
    "    parsed_dt = pd.to_datetime(cleaned, errors='coerce', utc=False)\n",
    "    method    = \"string_clean_parse\"\n",
    "else:\n",
    "    method = f\"epoch_unit_{unit_used}\"\n",
    "\n",
    "# Fallback: try alternate timestamp columns\n",
    "if parsed_dt.isna().sum() >= len(parsed_dt) * 0.99:\n",
    "    for alt in ['timestamp_ist', 'timestamp_utc', 'time', 'datetime']:\n",
    "        if alt in trades.columns and alt != timestamp_col:\n",
    "            parsed_alt, alt_unit = try_parse_epoch_unit(trades[alt])\n",
    "            if parsed_alt is not None and parsed_alt.notna().sum() > 0:\n",
    "                parsed_dt, method = parsed_alt, f\"alt_epoch_{alt}_{alt_unit}\"\n",
    "                break\n",
    "            parsed_alt = pd.to_datetime(\n",
    "                clean_timestamp_strings(trades[alt]), errors='coerce', utc=False)\n",
    "            if parsed_alt.notna().sum() > 0:\n",
    "                parsed_dt, method = parsed_alt, f\"alt_string_{alt}\"\n",
    "                break\n",
    "\n",
    "trades['datetime'] = parsed_dt\n",
    "print(f\"\\nTimestamp parsing method: {method}\")\n",
    "print(\"Parsed non-null:\", trades['datetime'].notna().sum(), \"of\", len(trades))\n",
    "if trades['datetime'].notna().sum() > 0:\n",
    "    print(\"Sample datetimes:\", trades['datetime'].head(5).tolist())\n",
    "    print(\"Range:\", trades['datetime'].min(), \"->\", trades['datetime'].max())\n",
    "\n",
    "\n",
    "if trades['datetime'].notna().sum() > 0:\n",
    "    n_1970   = (trades['datetime'].dt.year == 1970).sum()\n",
    "    frac_1970 = n_1970 / max(1, trades['datetime'].notna().sum())\n",
    "    if frac_1970 > 0.4:\n",
    "        print(f\"  [WARN] {n_1970} rows are year 1970 ({frac_1970:.1%}); re-parsing with best numeric unit...\")\n",
    "        snum = pd.to_numeric(trades[timestamp_col], errors='coerce').dropna()\n",
    "        if len(snum) > 0:\n",
    "            best_unit, best_valid = None, -1\n",
    "            for u in ['ns', 'us', 'ms', 's']:\n",
    "                try:\n",
    "                    dt_try = pd.to_datetime(snum.values.astype('int64'), unit=u, errors='coerce')\n",
    "                    valid  = pd.Series(dt_try).dt.year.between(2009, 2035).sum()\n",
    "                    if valid > best_valid:\n",
    "                        best_unit, best_valid = u, valid\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if best_unit:\n",
    "                trades['datetime'] = pd.to_datetime(\n",
    "                    pd.to_numeric(trades[timestamp_col], errors='coerce')\n",
    "                      .values.astype('float64').astype('int64'),\n",
    "                    unit=best_unit, errors='coerce'\n",
    "                )\n",
    "                print(\"  Reparsed unit:\", best_unit, \"| valid count:\", best_valid)\n",
    "                print(\"  New range:\", trades['datetime'].min(), \"->\", trades['datetime'].max())\n",
    "\n",
    "trades['date']  = trades['datetime'].dt.date\n",
    "before_drop     = len(trades)\n",
    "trades          = trades.dropna(subset=['date']).reset_index(drop=True)\n",
    "print(f\"Dropped {before_drop - len(trades)} rows (invalid datetime). Remaining: {len(trades)}\")\n",
    "print(\"Trades date range:\", trades['date'].min(), \"->\", trades['date'].max())\n",
    "\n",
    "# Rename to canonical column names\n",
    "rename_map = {}\n",
    "if acct_col        and acct_col        != 'account':     rename_map[acct_col]        = 'account'\n",
    "if closed_pnl_col  and closed_pnl_col  != 'closed_pnl':  rename_map[closed_pnl_col]  = 'closed_pnl'\n",
    "if size_usd_col    and size_usd_col    != 'size_usd':    rename_map[size_usd_col]    = 'size_usd'\n",
    "if size_tokens_col and size_tokens_col != 'size_tokens': rename_map[size_tokens_col] = 'size_tokens'\n",
    "if size_col_any    and size_col_any not in ['size_usd','size_tokens','size']:\n",
    "    rename_map[size_col_any] = 'size'\n",
    "if side_col        and side_col        != 'side':        rename_map[side_col]        = 'side'\n",
    "if leverage_col    and leverage_col in trades.columns and leverage_col != 'leverage':\n",
    "    rename_map[leverage_col] = 'leverage'\n",
    "\n",
    "trades.rename(columns=rename_map, inplace=True)\n",
    "canonical = [c for c in ['account','closed_pnl','size_usd','size_tokens','size',\n",
    "                          'datetime','date','side','leverage'] if c in trades.columns]\n",
    "print(\"\\nCanonical columns present:\", canonical)\n",
    "\n",
    "trades['is_win'] = trades['closed_pnl'] > 0\n",
    "if   'size_usd'    in trades.columns: trades['abs_size'] = trades['size_usd'].abs()\n",
    "elif 'size_tokens' in trades.columns: trades['abs_size'] = trades['size_tokens'].abs()\n",
    "elif 'size'        in trades.columns: trades['abs_size'] = trades['size'].abs()\n",
    "else:                                  trades['abs_size'] = np.nan\n",
    "\n",
    "#  Daily per-account aggregation\n",
    "agg_dict = {\n",
    "    'daily_pnl':       ('closed_pnl', 'sum'),\n",
    "    'trades_count':    ('closed_pnl', 'count'),\n",
    "    'win_count':       ('is_win',     'sum'),\n",
    "    'avg_trade_size':  ('abs_size',   'mean'),\n",
    "}\n",
    "if 'leverage' in trades.columns:\n",
    "    agg_dict['avg_leverage'] = ('leverage', 'mean')\n",
    "\n",
    "daily_account = trades.groupby(['date', 'account']).agg(**agg_dict).reset_index()\n",
    "\n",
    "# Long / short counts\n",
    "if 'side' in trades.columns:\n",
    "    def count_side(s, positives):\n",
    "        return s.astype(str).str.lower().isin(positives).sum()\n",
    "\n",
    "    longs  = (trades.groupby(['date','account'])['side']\n",
    "              .apply(lambda s: count_side(s, {'buy','long'}))    # FIX: no include_groups needed\n",
    "              .reset_index(name='long_count'))\n",
    "    shorts = (trades.groupby(['date','account'])['side']\n",
    "              .apply(lambda s: count_side(s, {'sell','short'}))\n",
    "              .reset_index(name='short_count'))\n",
    "    daily_account = daily_account.merge(longs,  on=['date','account'], how='left')\n",
    "    daily_account = daily_account.merge(shorts, on=['date','account'], how='left')\n",
    "else:\n",
    "    daily_account['long_count']  = np.nan\n",
    "    daily_account['short_count'] = np.nan\n",
    "\n",
    "daily_account['win_rate']         = daily_account['win_count']  / daily_account['trades_count']\n",
    "daily_account['long_short_ratio'] = daily_account['long_count'] / (daily_account['short_count'] + 1e-9)\n",
    "\n",
    "# Intra-day drawdown proxy\n",
    "def day_drawdown(sub):\n",
    "    sub  = sub.sort_values('datetime')\n",
    "    csum = sub['closed_pnl'].cumsum()\n",
    "    return csum.min() if not csum.empty else np.nan\n",
    "\n",
    "# FIX: use include_groups=False to suppress FutureWarning\n",
    "dd = (trades.groupby(['date','account'])\n",
    "      .apply(day_drawdown, include_groups=False)     # pandas ≥ 2.2 compat\n",
    "      .reset_index(name='daily_min_cum_pnl'))\n",
    "daily_account = daily_account.merge(dd, on=['date','account'], how='left')\n",
    "\n",
    "# ------- Platform / day aggregation -------\n",
    "platform_agg = {\n",
    "    'total_pnl':      ('daily_pnl',        'sum'),\n",
    "    'avg_win_rate':   ('win_rate',          'mean'),\n",
    "    'avg_trade_size': ('avg_trade_size',    'mean'),\n",
    "    'total_trades':   ('trades_count',      'sum'),\n",
    "    'avg_long_short': ('long_short_ratio',  'mean'),\n",
    "}\n",
    "if 'avg_leverage' in daily_account.columns:\n",
    "    platform_agg['avg_leverage'] = ('avg_leverage', 'mean')\n",
    "\n",
    "daily = daily_account.groupby('date').agg(**platform_agg).reset_index()\n",
    "daily = daily.merge(sent, on='date', how='left')\n",
    "\n",
    "# ------- Print Part A Summary -------\n",
    "print(\"\\n--- Part A Summary ---\")\n",
    "print(\"Trades:          rows,cols:\", trades.shape)\n",
    "print(\"Daily_account:   rows,cols:\", daily_account.shape)\n",
    "print(\"Daily (platform):rows,cols:\", daily.shape)\n",
    "\n",
    "print(\"\\nMissing values (trades) top 10:\")\n",
    "print(trades.isnull().sum().sort_values(ascending=False).head(10).to_string())\n",
    "\n",
    "print(\"\\nMissing values (daily_account) top 10:\")\n",
    "print(daily_account.isnull().sum().sort_values(ascending=False).head(10).to_string())\n",
    "\n",
    "print(\"\\nDuplicates: trades =\",        trades.duplicated().sum())\n",
    "print(\"Duplicates: daily_account =\",  daily_account.duplicated().sum())\n",
    "\n",
    "if 'leverage' in trades.columns:\n",
    "    print(\"\\nLeverage summary (trades):\")\n",
    "    print(trades['leverage'].describe().to_string())\n",
    "\n",
    "acct_summary = (daily_account.groupby('account')\n",
    "                .agg(total_pnl=('daily_pnl','sum'),\n",
    "                     total_trades=('trades_count','sum'),\n",
    "                     avg_win_rate=('win_rate','mean'))\n",
    "                .reset_index())\n",
    "print(\"\\nTop 5 accounts by total_pnl:\")\n",
    "print(acct_summary.sort_values('total_pnl', ascending=False).head(5).to_string(index=False))\n",
    "\n",
    "# ------- Save outputs -------\n",
    "daily_account.to_csv(\"outputs/daily_account_metrics.csv\", index=False)\n",
    "daily.to_csv(\"outputs/daily_metrics.csv\",         index=False)\n",
    "trades.to_csv(\"outputs/trades_cleaned.csv\",        index=False)\n",
    "\n",
    "print(\"\\nSaved to outputs/: daily_account_metrics.csv, daily_metrics.csv, trades_cleaned.csv\")\n",
    "# END PART A (\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d8cf2d-9f62-4067-a440-40807e39dedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_account: (102, 12)\n",
      "daily platform: (7, 8)\n",
      "trades: (211224, 20)\n",
      "Sentiment value counts:\n",
      "sentiment\n",
      "Greed            32\n",
      "Fear             32\n",
      "Nan              25\n",
      "Neutral           8\n",
      "Extreme greed     5\n",
      "\n",
      "--- Metric: daily_pnl ---\n",
      "{'fear_mean': 209372.66220543752, 'greed_mean': 99675.5167305, 'fear_median': 81389.6825155, 'greed_median': 35988.3764365, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 669.0, 'mw_p': 0.035589192227433085, 't_stat': 1.3092672646160282, 't_p': 0.19567563543239797}\n",
      "\n",
      "--- Metric: win_rate ---\n",
      "{'fear_mean': 0.4158784540695092, 'greed_mean': 0.37407443523093875, 'fear_median': 0.3939621144568476, 'greed_median': 0.4125989656493423, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 535.0, 'mw_p': 0.7625384725608533, 't_stat': 0.7648625894754894, 't_p': 0.448320925708963}\n",
      "\n",
      "--- Metric: daily_min_cum_pnl ---\n",
      "{'fear_mean': -10431.168132499999, 'greed_mean': -10294.545593593753, 'fear_median': 0.0, 'greed_median': 0.0, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 411.0, 'mw_p': 0.13302672146649336, 't_stat': -0.012034131775842756, 't_p': 0.990452075130194}\n",
      "\n",
      "--- Metric: avg_leverage ---\n",
      "{'fear_mean': 14001.822141061175, 'greed_mean': 19910.93700735995, 'fear_median': 17.542345274140974, 'greed_median': 2306.3549632171807, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 424.0, 'mw_p': 0.24004434259843443, 't_stat': -0.22909973473165596, 't_p': 0.8198617107061527}\n",
      "Saved boxplot_daily_pnl_by_sentiment.png\n",
      "Saved boxplot_win_rate_by_sentiment.png\n",
      "Saved ts_total_pnl_sentiment.png\n",
      "\n",
      "--- Behavior metric: trades_count ---\n",
      "{'fear_mean': 4183.46875, 'greed_mean': 1134.03125, 'fear_median': 2763.5, 'greed_median': 275.0, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 807.0, 'mw_p': 7.676006943746331e-05, 't_stat': 2.81998088338073, 't_p': 0.007474847250943747}\n",
      "\n",
      "--- Behavior metric: avg_leverage ---\n",
      "{'fear_mean': 14001.822141061175, 'greed_mean': 19910.93700735995, 'fear_median': 17.542345274140974, 'greed_median': 2306.3549632171807, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 424.0, 'mw_p': 0.24004434259843443, 't_stat': -0.22909973473165596, 't_p': 0.8198617107061527}\n",
      "\n",
      "--- Behavior metric: avg_trade_size ---\n",
      "{'fear_mean': 5926.522722732153, 'greed_mean': 5839.310973510808, 'fear_median': 3207.887908866018, 'greed_median': 2709.5321978345146, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 547.0, 'mw_p': 0.6431946345753736, 't_stat': 0.04849021723287926, 't_p': 0.9614825519288503}\n",
      "\n",
      "--- Behavior metric: long_short_ratio ---\n",
      "{'fear_mean': 0.9681304304525933, 'greed_mean': 7593750001.130221, 'fear_median': 0.8916383395835117, 'greed_median': 0.8817817613585619, 'n_fear': 32, 'n_greed': 32, 'mw_stat': 507.0, 'mw_p': 0.9518171803497951, 't_stat': -1.0902888383904383, 't_p': 0.2839885448220707}\n",
      "\n",
      "Saved agg_behav_by_sentiment.csv\n",
      "    sentiment  mean_trades  median_trades  mean_leverage\n",
      "Extreme greed   1392.40000          730.0  109102.060674\n",
      "         Fear   4183.46875         2763.5   14001.822141\n",
      "        Greed   1134.03125          275.0   19910.937007\n",
      "          Nan   1078.44000          198.0 -104343.295540\n",
      "      Neutral    892.62500          164.5   42099.597407\n",
      "Saved pct_long_by_sentiment.csv\n",
      "    sentiment  pct_long\n",
      "Extreme greed  0.484200\n",
      "         Fear  0.493617\n",
      "        Greed  0.424950\n",
      "          Nan  0.531063\n",
      "      Neutral  0.490828\n",
      "\n",
      "Clustering on features: ['avg_leverage', 'avg_trades_per_day', 'avg_win_rate']\n",
      "Saved account_segments.csv and cluster_summary.csv\n",
      "\n",
      "Cluster summary (means):\n",
      "          avg_leverage  avg_trades_per_day  avg_win_rate      total_pnl\n",
      "cluster                                                                \n",
      "0         24145.189812         1532.886139      0.383982  308729.239284\n",
      "1         28664.002767        11995.333333      0.586406  888122.179648\n",
      "2       -445764.113390         2641.583333      0.276284  -61852.057903\n",
      "Saved cluster_scatter.png\n",
      "Saved sentiment_metric_tests.csv\n",
      "\n",
      "✅ PART B COMPLETE — Share your Part C code when ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART B  Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   # FIX: non-interactive backend (safe for scripts & notebooks)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------- Config / load -------\n",
    "DAILY_ACCT = \"outputs/daily_account_metrics.csv\"\n",
    "DAILY      = \"outputs/daily_metrics.csv\"\n",
    "TRADES     = \"outputs/trades_cleaned.csv\"\n",
    "\n",
    "daily_account = pd.read_csv(DAILY_ACCT, parse_dates=['date'])\n",
    "daily         = pd.read_csv(DAILY,      parse_dates=['date'])\n",
    "trades        = pd.read_csv(TRADES,     parse_dates=['datetime','date'], low_memory=False)\n",
    "\n",
    "print(\"daily_account:\", daily_account.shape)\n",
    "print(\"daily platform:\", daily.shape)\n",
    "print(\"trades:\", trades.shape)\n",
    "\n",
    "# ------- Normalize sentiment -------\n",
    "daily['sentiment'] = daily['sentiment'].astype(str).str.strip().str.capitalize()\n",
    "\n",
    "# FIX: safe sentiment merge — drop existing sentiment col in daily_account first to avoid duplication\n",
    "if 'sentiment' in daily_account.columns:\n",
    "    daily_account.drop(columns=['sentiment'], inplace=True)\n",
    "\n",
    "sent_map = daily.set_index('date')['sentiment'].to_dict()\n",
    "daily_account['sentiment'] = daily_account['date'].map(sent_map)\n",
    "\n",
    "print(\"Sentiment value counts:\")\n",
    "print(daily_account['sentiment'].value_counts().to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 1) Performance: Fear vs Greed days\n",
    "# ============================================================\n",
    "metrics = ['daily_pnl', 'win_rate', 'daily_min_cum_pnl']\n",
    "if 'avg_leverage' in daily_account.columns:\n",
    "    metrics.append('avg_leverage')\n",
    "\n",
    "compare_df = daily_account.dropna(subset=['sentiment']).copy()\n",
    "\n",
    "def compare_metric(metric, df=None):\n",
    "    if df is None:\n",
    "        df = compare_df\n",
    "    # FIX: guard against missing column\n",
    "    if metric not in df.columns:\n",
    "        print(f\"  [SKIP] Column '{metric}' not found.\")\n",
    "        return {}, np.array([]), np.array([])\n",
    "\n",
    "    grp   = df.groupby('sentiment')[metric].apply(list).to_dict()\n",
    "    fear  = np.array([x for x in grp.get('Fear',  []) if pd.notna(x)])\n",
    "    greed = np.array([x for x in grp.get('Greed', []) if pd.notna(x)])\n",
    "\n",
    "    summary = {\n",
    "        'fear_mean':   np.nanmean(fear)   if len(fear)  > 0 else np.nan,\n",
    "        'greed_mean':  np.nanmean(greed)  if len(greed) > 0 else np.nan,\n",
    "        'fear_median': np.nanmedian(fear) if len(fear)  > 0 else np.nan,\n",
    "        'greed_median':np.nanmedian(greed)if len(greed) > 0 else np.nan,\n",
    "        'n_fear':  len(fear),\n",
    "        'n_greed': len(greed),\n",
    "    }\n",
    "\n",
    "    # FIX: only run tests if both groups have data\n",
    "    mw_stat = mw_p = t_stat = t_p = np.nan\n",
    "    if len(fear) > 1 and len(greed) > 1:\n",
    "        try:\n",
    "            mw_stat, mw_p = stats.mannwhitneyu(fear, greed, alternative='two-sided')\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            t_stat, t_p = stats.ttest_ind(fear, greed, equal_var=False, nan_policy='omit')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    summary.update({'mw_stat': mw_stat, 'mw_p': mw_p, 't_stat': t_stat, 't_p': t_p})\n",
    "    return summary, fear, greed\n",
    "\n",
    "summaries = {}\n",
    "for m in metrics:\n",
    "    s, f, g = compare_metric(m)\n",
    "    summaries[m] = s\n",
    "    print(f\"\\n--- Metric: {m} ---\")\n",
    "    print(s)\n",
    "\n",
    "# ------- Boxplot: daily_pnl by sentiment -------\n",
    "fear_pnl  = compare_df.loc[compare_df['sentiment'] == 'Fear',  'daily_pnl'].dropna()\n",
    "greed_pnl = compare_df.loc[compare_df['sentiment'] == 'Greed', 'daily_pnl'].dropna()\n",
    "\n",
    "# FIX: only plot if both groups are non-empty\n",
    "if len(fear_pnl) > 0 and len(greed_pnl) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.boxplot([fear_pnl, greed_pnl], labels=['Fear', 'Greed'])\n",
    "    ax.set_ylabel('daily_pnl (per-account)')\n",
    "    ax.set_title('Distribution of daily_pnl by Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/boxplot_daily_pnl_by_sentiment.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved boxplot_daily_pnl_by_sentiment.png\")\n",
    "else:\n",
    "    print(\"[SKIP] Not enough data for daily_pnl boxplot.\")\n",
    "\n",
    "# ------- Boxplot: win_rate by sentiment -------\n",
    "fear_wr  = compare_df.loc[compare_df['sentiment'] == 'Fear',  'win_rate'].dropna()\n",
    "greed_wr = compare_df.loc[compare_df['sentiment'] == 'Greed', 'win_rate'].dropna()\n",
    "\n",
    "if len(fear_wr) > 0 and len(greed_wr) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.boxplot([fear_wr, greed_wr], labels=['Fear', 'Greed'])\n",
    "    ax.set_ylabel('win_rate (daily, per-account)')\n",
    "    ax.set_title('Win Rate by Sentiment')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/boxplot_win_rate_by_sentiment.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved boxplot_win_rate_by_sentiment.png\")\n",
    "else:\n",
    "    print(\"[SKIP] Not enough data for win_rate boxplot.\")\n",
    "\n",
    "# ------- Timeseries: platform PnL + sentiment markers -------\n",
    "if 'total_pnl' in daily.columns and daily['total_pnl'].notna().sum() > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    daily_sorted = daily.sort_values('date')\n",
    "    ax.plot(daily_sorted['date'],\n",
    "            daily_sorted['total_pnl'].rolling(7, min_periods=1).mean(),\n",
    "            label='7-day MA total_pnl', color='steelblue')\n",
    "\n",
    "    fear_days  = daily_sorted[daily_sorted['sentiment'] == 'Fear']\n",
    "    greed_days = daily_sorted[daily_sorted['sentiment'] == 'Greed']\n",
    "    if len(fear_days)  > 0:\n",
    "        ax.scatter(fear_days['date'],  fear_days['total_pnl'],  c='red',   s=12, label='Fear days',  zorder=3)\n",
    "    if len(greed_days) > 0:\n",
    "        ax.scatter(greed_days['date'], greed_days['total_pnl'], c='green', s=12, label='Greed days', zorder=3)\n",
    "\n",
    "    ax.legend(); ax.set_title('Platform total_pnl (7d MA) with Fear/Greed markers')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"outputs/ts_total_pnl_sentiment.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved ts_total_pnl_sentiment.png\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Behavior change metrics\n",
    "# ============================================================\n",
    "behav_daily = daily_account.copy()\n",
    "behavior_metrics = [m for m in ['trades_count','avg_leverage','avg_trade_size','long_short_ratio']\n",
    "                    if m in behav_daily.columns]\n",
    "\n",
    "for m in behavior_metrics:\n",
    "    s, f, g = compare_metric(m, df=behav_daily)\n",
    "    summaries[m] = s\n",
    "    print(f\"\\n--- Behavior metric: {m} ---\")\n",
    "    print(s)\n",
    "\n",
    "agg_behav = (behav_daily.groupby('sentiment')\n",
    "             .agg(mean_trades  =('trades_count', 'mean'),\n",
    "                  median_trades=('trades_count', 'median'),\n",
    "                  **({'mean_leverage': ('avg_leverage','mean')} if 'avg_leverage' in behav_daily.columns else {}))\n",
    "             .reset_index())\n",
    "agg_behav.to_csv(\"outputs/agg_behav_by_sentiment.csv\", index=False)\n",
    "print(\"\\nSaved agg_behav_by_sentiment.csv\")\n",
    "print(agg_behav.to_string(index=False))\n",
    "\n",
    "# ------- Long/Short bias -------\n",
    "if 'side' in trades.columns:\n",
    "    # FIX: drop sentiment from trades first to avoid column collision on merge\n",
    "    if 'sentiment' in trades.columns:\n",
    "        trades.drop(columns=['sentiment'], inplace=True)\n",
    "\n",
    "    trades['side_norm'] = trades['side'].astype(str).str.lower().str.strip()\n",
    "    trades['is_long']   = trades['side_norm'].isin(['buy', 'long'])\n",
    "\n",
    "    # FIX: merge only date+sentiment, avoid duplicate cols\n",
    "    trades = trades.merge(\n",
    "        daily[['date','sentiment']].drop_duplicates(),\n",
    "        on='date', how='left'\n",
    "    )\n",
    "    ls = trades.groupby('sentiment')['is_long'].mean().reset_index(name='pct_long')\n",
    "    ls.to_csv(\"outputs/pct_long_by_sentiment.csv\", index=False)\n",
    "    print(\"Saved pct_long_by_sentiment.csv\")\n",
    "    print(ls.to_string(index=False))\n",
    "else:\n",
    "    print(\"No 'side' column — cannot compute long/short bias.\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Account clustering (K-Means, k=3)\n",
    "# ============================================================\n",
    "agg_kwargs = dict(\n",
    "    total_pnl          =('daily_pnl',       'sum'),\n",
    "    avg_win_rate        =('win_rate',        'mean'),\n",
    "    avg_trades_per_day  =('trades_count',    'mean'),\n",
    "    avg_size            =('avg_trade_size',  'mean'),\n",
    "    days_active         =('date',            'nunique'),\n",
    ")\n",
    "if 'avg_leverage' in daily_account.columns:\n",
    "    agg_kwargs['avg_leverage'] = ('avg_leverage', 'mean')\n",
    "\n",
    "acct_feat = (daily_account.groupby('account')\n",
    "             .agg(**agg_kwargs)\n",
    "             .reset_index()\n",
    "             .replace([np.inf, -np.inf], np.nan)\n",
    "             .fillna(0))\n",
    "\n",
    "# Choose best available clustering features\n",
    "cluster_features = [f for f in ['avg_leverage','avg_trades_per_day','avg_win_rate']\n",
    "                    if f in acct_feat.columns]\n",
    "if len(cluster_features) < 2:\n",
    "    cluster_features = acct_feat.select_dtypes(include=[np.number]).columns.tolist()[:3]\n",
    "\n",
    "print(\"\\nClustering on features:\", cluster_features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X      = scaler.fit_transform(acct_feat[cluster_features].values)\n",
    "\n",
    "k      = min(3, len(acct_feat))   # FIX: guard if fewer than 3 accounts\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "acct_feat['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "cluster_summary = (acct_feat.groupby('cluster')[cluster_features + ['total_pnl','days_active']]\n",
    "                   .agg(['mean','median','count']))\n",
    "cluster_summary.to_csv(\"outputs/cluster_summary.csv\")\n",
    "acct_feat.to_csv(\"outputs/account_segments.csv\", index=False)\n",
    "print(\"Saved account_segments.csv and cluster_summary.csv\")\n",
    "print(\"\\nCluster summary (means):\")\n",
    "print(acct_feat.groupby('cluster')[cluster_features + ['total_pnl']].mean().to_string())\n",
    "\n",
    "# Cluster scatter plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x_feat = cluster_features[1] if len(cluster_features) > 1 else cluster_features[0]\n",
    "y_feat = cluster_features[0]\n",
    "for c in sorted(acct_feat['cluster'].unique()):\n",
    "    sub = acct_feat[acct_feat['cluster'] == c]\n",
    "    ax.scatter(sub[x_feat], sub[y_feat], s=20, label=f'Cluster {c}')\n",
    "ax.set_xlabel(x_feat); ax.set_ylabel(y_feat)\n",
    "ax.legend(); ax.set_title(f'Clusters ({x_feat} vs {y_feat})')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/cluster_scatter.png\", dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved cluster_scatter.png\")\n",
    "\n",
    "# ------- Save all summary stats -------\n",
    "pd.DataFrame.from_dict(summaries, orient='index').to_csv(\"outputs/sentiment_metric_tests.csv\")\n",
    "print(\"Saved sentiment_metric_tests.csv\")\n",
    "\n",
    "print(\"\\n✅ PART B COMPLETE — Share your Part C code when ready.\")\n",
    "# END PART B (FIXED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51219ee3-4f2f-40c2-b291-0a1222c5a791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in use:\n",
      "  TRADES: outputs/trades_cleaned.csv\n",
      "  DAILY: outputs/daily_metrics.csv\n",
      "  DAILY_ACCT: outputs/daily_account_metrics.csv\n",
      "  ACCT_SEG: outputs/account_segments.csv\n",
      "Detected size_col: size_usd | leverage_col: leverage\n",
      "\n",
      "Cluster means:\n",
      " cluster   avg_leverage  avg_win_rate  avg_trades_per_day\n",
      "       0   24145.189812      0.383982         1532.886139\n",
      "       1   28664.002767      0.586406        11995.333333\n",
      "       2 -445764.113390      0.276284         2641.583333\n",
      "\n",
      "Clusters → high_leverage: 1 | consistent_winners: 0\n",
      "\n",
      "SIMULATION SUMMARY\n",
      "------------------\n",
      "Total orig pnl : 10,296,958.94\n",
      "Total adj  pnl : 9,496,464.51\n",
      "Delta          : -800,494.44\n",
      "Orig daily vol : 2,581,231.98\n",
      "Adj  daily vol : 2,313,607.15\n",
      "\n",
      "Top 5 accounts HURT:\n",
      "                                   account      orig_pnl       adj_pnl         delta\n",
      "0xbaaaf6571ab7d571043ff1e313a9609a10637864  9.401638e+05  1.880380e+05 -7.521258e+05\n",
      "0xbee1707d6b44d4d52bfe19e41f8a828645437aab  8.360806e+05  7.697635e+05 -6.631705e+04\n",
      "0xb1231a4a2dd02f2276fa3c5e2a2f3436e6bfed23  2.143383e+06  2.143383e+06 -2.100132e-07\n",
      "0x8170715b3b381dffb7062c0298972d4727a0a63b -1.676211e+05 -1.676211e+05 -1.841981e-07\n",
      "0x47add9a56df66b524d5e2c1993a43cde53b6ed85  1.033437e+05  1.033437e+05 -1.523731e-07\n",
      "\n",
      "Top 5 accounts HELPED:\n",
      "                                   account      orig_pnl       adj_pnl        delta\n",
      "0x430f09841d65beb3f27765503d0f850b8bce7713 416541.872341 416541.872341 0.000000e+00\n",
      "0x271b280974205ca63b716753467d5a371de622ab -70436.191318 -70436.191318 2.793968e-09\n",
      "0x8477e447846c758f5a675856001ea72298fd9cb5  43917.008976  43917.008976 1.180888e-08\n",
      "0x75f7eeb85dc639d5e99c78f95393aa9a5f1170d4 379095.406711 386600.673086 7.505266e+03\n",
      "0x2c229d22b100a7beb69122eed721cee9b24011dd 168658.004994 179101.150884 1.044315e+04\n",
      "\n",
      "Model dataset size: 102 rows\n",
      "Class distribution: {1: 59, 0: 43}\n",
      "Train rows: 81 | Test rows: 21\n",
      "\n",
      "PREDICTIVE MODEL SUMMARY\n",
      "------------------------\n",
      "Train rows : 81 | Test rows: 21\n",
      "Accuracy   : 0.7619\n",
      "F1 Score   : 0.8148\n",
      "AUC        : 0.8333\n",
      "CV AUC     : 0.7210 ± 0.1081\n",
      "Confusion matrix:\n",
      " [[ 5  4]\n",
      " [ 1 11]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.56      0.67         9\n",
      "           1       0.73      0.92      0.81        12\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.78      0.74      0.74        21\n",
      "weighted avg       0.78      0.76      0.75        21\n",
      "\n",
      "Saved model_feature_importances_partc_final.png\n",
      "\n",
      "All outputs saved to outputs/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART C (FIXED v2): Strategy Simulation + Predictive Model\n",
    "# Fixes over v1:\n",
    "#   1. Train/test split by ROW INDEX not by date → more training data\n",
    "#   2. Separated high_leverage and consistent_winners clusters\n",
    "#   3. Added 14-day rolling features for better signal\n",
    "#   4. Added SMOTE-style class balancing via class_weight\n",
    "#   5. Lowered prediction threshold (0.45) to improve class 0 recall\n",
    "#   6. Added cross-validation score for reliability check\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "LEVERAGE_CAP      = 3.0\n",
    "SIZE_INCREASE_PCT = 0.20\n",
    "LEVERAGE_CLIP_PCT = 0.99\n",
    "SCALE_MIN, SCALE_MAX = 0.2, 5.0\n",
    "RANDOM_STATE      = 42\n",
    "PRED_THRESHOLD    = 0.45   # lower threshold to improve class 0 detection\n",
    "# ------------------------------------------------\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "def pick(*paths):\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "TRADES_FILE     = pick(\"outputs/trades_cleaned.csv\",       \"outputs/trades.csv\")\n",
    "DAILY_FILE      = pick(\"outputs/daily_metrics.csv\",        \"outputs/daily.csv\")\n",
    "DAILY_ACCT_FILE = pick(\"outputs/daily_account_metrics.csv\")\n",
    "ACCT_SEG_FILE   = pick(\"outputs/account_segments.csv\",     \"outputs/account_segments_fixed.csv\")\n",
    "\n",
    "missing = [n for n, f in [(\"TRADES\", TRADES_FILE), (\"DAILY\", DAILY_FILE),\n",
    "                            (\"DAILY_ACCT\", DAILY_ACCT_FILE), (\"ACCT_SEG\", ACCT_SEG_FILE)] if not f]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Required output files missing: {missing}. Run Part A & B first.\")\n",
    "\n",
    "print(\"Files in use:\")\n",
    "for label, path in [(\"TRADES\", TRADES_FILE), (\"DAILY\", DAILY_FILE),\n",
    "                     (\"DAILY_ACCT\", DAILY_ACCT_FILE), (\"ACCT_SEG\", ACCT_SEG_FILE)]:\n",
    "    print(f\"  {label}: {path}\")\n",
    "\n",
    "# -------------------- Load --------------------\n",
    "trades        = pd.read_csv(TRADES_FILE,     low_memory=False)\n",
    "daily         = pd.read_csv(DAILY_FILE,      low_memory=False)\n",
    "daily_account = pd.read_csv(DAILY_ACCT_FILE, low_memory=False)\n",
    "acct_seg      = pd.read_csv(ACCT_SEG_FILE,   low_memory=False)\n",
    "\n",
    "# -------------------- Normalize dates --------------------\n",
    "def to_date_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            parsed = pd.to_datetime(df[c], errors='coerce')\n",
    "            if parsed.notna().sum() > 0:\n",
    "                return parsed.dt.normalize()\n",
    "    raise ValueError(\"No parsable date column found among: \" + str(candidates))\n",
    "\n",
    "daily['date']         = to_date_col(daily,         ['date', 'day'])\n",
    "daily_account['date'] = to_date_col(daily_account, ['date'])\n",
    "\n",
    "if 'date' in trades.columns:\n",
    "    trades['date'] = to_date_col(trades, ['date'])\n",
    "elif 'datetime' in trades.columns:\n",
    "    trades['date'] = pd.to_datetime(trades['datetime'], errors='coerce').dt.normalize()\n",
    "else:\n",
    "    raise ValueError(\"trades has neither 'date' nor 'datetime' column.\")\n",
    "\n",
    "# -------------------- Validate --------------------\n",
    "if 'account' not in trades.columns or 'account' not in acct_seg.columns:\n",
    "    raise ValueError(\"Missing 'account' column in trades or account_segments.\")\n",
    "\n",
    "# -------------------- Numeric conversions --------------------\n",
    "trades['closed_pnl'] = pd.to_numeric(\n",
    "    trades.get('closed_pnl', trades.get('pnl', pd.Series(0, index=trades.index))),\n",
    "    errors='coerce').fillna(0.0)\n",
    "\n",
    "size_col = next((c for c in ['size_usd','size','size_tokens','abs_size','notional']\n",
    "                 if c in trades.columns), None)\n",
    "lev_col  = next((c for c in ['leverage','lev','margin','leverage_ratio']\n",
    "                 if c in trades.columns), None)\n",
    "\n",
    "print(\"Detected size_col:\", size_col, \"| leverage_col:\", lev_col)\n",
    "\n",
    "trades['old_leverage'] = pd.to_numeric(trades[lev_col],  errors='coerce') if lev_col  else np.nan\n",
    "trades['old_size']     = pd.to_numeric(trades[size_col], errors='coerce') if size_col else np.nan\n",
    "\n",
    "trades.loc[trades['old_leverage'] == 0, 'old_leverage'] = np.nan\n",
    "trades.loc[trades['old_size']     == 0, 'old_size']     = np.nan\n",
    "\n",
    "if trades['old_leverage'].notna().sum() > 0:\n",
    "    lev_cap_val = trades['old_leverage'].quantile(LEVERAGE_CLIP_PCT)\n",
    "    trades['old_leverage_clipped'] = trades['old_leverage'].clip(lower=1.0, upper=lev_cap_val)\n",
    "else:\n",
    "    trades['old_leverage_clipped'] = 1.0\n",
    "\n",
    "trades['old_size_filled']     = trades['old_size'].fillna(1.0)\n",
    "trades['old_leverage_filled'] = trades['old_leverage_clipped'].fillna(1.0)\n",
    "\n",
    "# -------------------- Merge cluster --------------------\n",
    "acct_seg_small = acct_seg[['account','cluster']].drop_duplicates()\n",
    "trades = trades.merge(acct_seg_small, on='account', how='left')\n",
    "trades['cluster'] = trades['cluster'].fillna(-1).astype(int)\n",
    "\n",
    "# -------------------- Identify clusters --------------------\n",
    "feat_for_cluster = [f for f in ['avg_leverage','avg_win_rate','avg_trades_per_day']\n",
    "                    if f in acct_seg.columns]\n",
    "if not feat_for_cluster:\n",
    "    feat_for_cluster = acct_seg.select_dtypes(include=[np.number]).columns[:2].tolist()\n",
    "\n",
    "cluster_means = acct_seg.groupby('cluster')[feat_for_cluster].mean().reset_index()\n",
    "print(\"\\nCluster means:\")\n",
    "print(cluster_means.to_string(index=False))\n",
    "\n",
    "# High leverage cluster\n",
    "lev_sort_col = 'avg_leverage' if 'avg_leverage' in cluster_means.columns else feat_for_cluster[0]\n",
    "sorted_by_lev = cluster_means.sort_values(lev_sort_col, ascending=False)\n",
    "high_leverage_cluster = int(sorted_by_lev.iloc[0]['cluster'])\n",
    "\n",
    "# FIX: consistent winners must be a DIFFERENT cluster from high_leverage\n",
    "if 'avg_win_rate' in cluster_means.columns:\n",
    "    sorted_by_wr = cluster_means.sort_values('avg_win_rate', ascending=False)\n",
    "    for _, row in sorted_by_wr.iterrows():\n",
    "        if int(row['cluster']) != high_leverage_cluster:\n",
    "            consistent_winners_cluster = int(row['cluster'])\n",
    "            break\n",
    "    else:\n",
    "        consistent_winners_cluster = int(sorted_by_wr.iloc[0]['cluster'])\n",
    "elif 'total_pnl' in acct_seg.columns:\n",
    "    pnl_means = acct_seg.groupby('cluster')['total_pnl'].mean().reset_index().sort_values('total_pnl', ascending=False)\n",
    "    for _, row in pnl_means.iterrows():\n",
    "        if int(row['cluster']) != high_leverage_cluster:\n",
    "            consistent_winners_cluster = int(row['cluster'])\n",
    "            break\n",
    "else:\n",
    "    all_clusters = cluster_means['cluster'].tolist()\n",
    "    consistent_winners_cluster = next((c for c in all_clusters if c != high_leverage_cluster),\n",
    "                                       high_leverage_cluster)\n",
    "\n",
    "print(f\"\\nClusters → high_leverage: {high_leverage_cluster} | consistent_winners: {consistent_winners_cluster}\")\n",
    "\n",
    "# -------------------- Map sentiment --------------------\n",
    "if 'sentiment' in trades.columns:\n",
    "    trades.drop(columns=['sentiment'], inplace=True)\n",
    "\n",
    "sent_map = daily.set_index('date')['sentiment'].to_dict()\n",
    "trades['date_norm'] = pd.to_datetime(trades['date']).dt.normalize()\n",
    "trades['sentiment'] = trades['date_norm'].map(sent_map).ffill().bfill()\n",
    "\n",
    "# -------------------- Apply strategy rules --------------------\n",
    "mask_r1 = ((trades['cluster'] == high_leverage_cluster) &\n",
    "            (trades['sentiment'].str.lower() == 'fear'))\n",
    "trades['new_leverage'] = trades['old_leverage_filled'].copy()\n",
    "trades.loc[mask_r1, 'new_leverage'] = np.minimum(\n",
    "    trades.loc[mask_r1, 'old_leverage_filled'], LEVERAGE_CAP)\n",
    "\n",
    "acct_win = (daily_account.groupby('account')['win_rate']\n",
    "            .mean().reset_index().rename(columns={'win_rate': 'acct_win_rate'}))\n",
    "trades = trades.merge(acct_win, on='account', how='left')\n",
    "\n",
    "mask_r2 = ((trades['cluster'] == consistent_winners_cluster) &\n",
    "            (trades['sentiment'].str.lower() == 'greed') &\n",
    "            (trades['acct_win_rate'] > 0.5))\n",
    "trades['new_size'] = trades['old_size_filled'].copy()\n",
    "trades.loc[mask_r2, 'new_size'] = trades.loc[mask_r2, 'old_size_filled'] * (1 + SIZE_INCREASE_PCT)\n",
    "\n",
    "trades['new_leverage'] = trades['new_leverage'].fillna(trades['old_leverage_filled'])\n",
    "trades['new_size']     = trades['new_size'].fillna(trades['old_size_filled'])\n",
    "\n",
    "# -------------------- Scaling + adjusted PnL --------------------\n",
    "epsilon = 1e-9\n",
    "trades['scale_raw']      = ((trades['new_leverage'] * trades['new_size']) /\n",
    "                             (trades['old_leverage_filled'] * trades['old_size_filled'] + epsilon))\n",
    "trades['scale']          = trades['scale_raw'].clip(lower=SCALE_MIN, upper=SCALE_MAX)\n",
    "trades['closed_pnl_adj'] = trades['closed_pnl'] * trades['scale']\n",
    "\n",
    "# -------------------- Aggregate results --------------------\n",
    "orig_daily = trades.groupby('date_norm')['closed_pnl'].sum().reset_index(name='orig_total_pnl')\n",
    "adj_daily  = trades.groupby('date_norm')['closed_pnl_adj'].sum().reset_index(name='adj_total_pnl')\n",
    "compare_daily = (orig_daily.merge(adj_daily, on='date_norm', how='outer')\n",
    "                 .fillna(0).sort_values('date_norm').reset_index(drop=True))\n",
    "compare_daily['delta'] = compare_daily['adj_total_pnl'] - compare_daily['orig_total_pnl']\n",
    "\n",
    "tot_orig  = compare_daily['orig_total_pnl'].sum()\n",
    "tot_adj   = compare_daily['adj_total_pnl'].sum()\n",
    "delta_tot = tot_adj - tot_orig\n",
    "\n",
    "orig_acct = trades.groupby('account')['closed_pnl'].sum().reset_index(name='orig_pnl')\n",
    "adj_acct  = trades.groupby('account')['closed_pnl_adj'].sum().reset_index(name='adj_pnl')\n",
    "acct_compare = (orig_acct.merge(adj_acct, on='account', how='outer')\n",
    "                .fillna(0).assign(delta=lambda d: d['adj_pnl'] - d['orig_pnl'])\n",
    "                .sort_values('delta'))\n",
    "\n",
    "orig_vol = trades.groupby('date_norm')['closed_pnl'].sum().std()\n",
    "adj_vol  = trades.groupby('date_norm')['closed_pnl_adj'].sum().std()\n",
    "\n",
    "compare_daily.to_csv(\"outputs/strategy_simulation_daily_impact_partc_final.csv\", index=False)\n",
    "trades.to_csv(\"outputs/trades_sim_full_partc_final.csv\", index=False)\n",
    "acct_compare.to_csv(\"outputs/account_impact_partc_final.csv\", index=False)\n",
    "\n",
    "print(\"\\nSIMULATION SUMMARY\")\n",
    "print(\"------------------\")\n",
    "print(f\"Total orig pnl : {tot_orig:,.2f}\")\n",
    "print(f\"Total adj  pnl : {tot_adj:,.2f}\")\n",
    "print(f\"Delta          : {delta_tot:,.2f}\")\n",
    "print(f\"Orig daily vol : {orig_vol:,.2f}\")\n",
    "print(f\"Adj  daily vol : {adj_vol:,.2f}\")\n",
    "print(\"\\nTop 5 accounts HURT:\")\n",
    "print(acct_compare.head(5).to_string(index=False))\n",
    "print(\"\\nTop 5 accounts HELPED:\")\n",
    "print(acct_compare.tail(5).to_string(index=False))\n",
    "\n",
    "# Cumulative PnL plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(compare_daily['date_norm'], compare_daily['orig_total_pnl'].cumsum(), label='Original')\n",
    "ax.plot(compare_daily['date_norm'], compare_daily['adj_total_pnl'].cumsum(),  label='Adjusted')\n",
    "ax.set_title('Cumulative PnL: Original vs Strategy-Adjusted')\n",
    "ax.legend(); plt.tight_layout()\n",
    "plt.savefig(\"outputs/simulation_cumpnl_partc_final.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# Predictive Model — FIXED\n",
    "# ============================================================\n",
    "df = daily_account.copy()\n",
    "df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "df = df.sort_values(['account','date']).reset_index(drop=True)\n",
    "\n",
    "df['next_daily_pnl']         = df.groupby('account')['daily_pnl'].shift(-1)\n",
    "df['target_next_profitable'] = (df['next_daily_pnl'] > 0).astype(int)\n",
    "\n",
    "if 'sentiment' in df.columns:\n",
    "    df.drop(columns=['sentiment'], inplace=True)\n",
    "df = df.merge(daily[['date','sentiment']].drop_duplicates(), on='date', how='left')\n",
    "df['sentiment_num'] = df['sentiment'].map({'Fear': -1, 'Greed': 1}).fillna(0)\n",
    "\n",
    "for col in ['trades_count','avg_leverage','win_rate','avg_trade_size']:\n",
    "    df[col] = pd.to_numeric(df.get(col, 0), errors='coerce').fillna(0)\n",
    "\n",
    "# FIX: added 14-day rolling features for more signal\n",
    "for feat, src in [('rol_trades_7','trades_count'),   ('rol_leverage_7','avg_leverage'),\n",
    "                   ('rol_winrate_7','win_rate'),       ('rol_size_7','avg_trade_size'),\n",
    "                   ('rol_trades_14','trades_count'),   ('rol_leverage_14','avg_leverage'),\n",
    "                   ('rol_winrate_14','win_rate'),      ('rol_pnl_7','daily_pnl')]:\n",
    "    window = 14 if '14' in feat else 7\n",
    "    df[feat] = df.groupby('account')[src].transform(\n",
    "        lambda s: s.rolling(window, min_periods=1).mean())\n",
    "\n",
    "feat_cols = ['sentiment_num','rol_trades_7','rol_leverage_7','rol_winrate_7',\n",
    "             'rol_size_7','rol_trades_14','rol_leverage_14','rol_winrate_14','rol_pnl_7']\n",
    "\n",
    "df_model = df.dropna(subset=feat_cols + ['target_next_profitable']).copy().reset_index(drop=True)\n",
    "print(f\"\\nModel dataset size: {df_model.shape[0]} rows\")\n",
    "print(\"Class distribution:\", df_model['target_next_profitable'].value_counts().to_dict())\n",
    "\n",
    "if df_model.shape[0] < 20:\n",
    "    print(\"WARNING: dataset too small. Skipping model.\")\n",
    "else:\n",
    "    # FIX: split by ROW INDEX (80/20) not by date → guarantees enough training rows\n",
    "    split_idx  = int(len(df_model) * 0.80)\n",
    "    X_train    = df_model.loc[:split_idx-1, feat_cols].values\n",
    "    y_train    = df_model.loc[:split_idx-1, 'target_next_profitable'].values\n",
    "    X_test     = df_model.loc[split_idx:,   feat_cols].values\n",
    "    y_test     = df_model.loc[split_idx:,   'target_next_profitable'].values\n",
    "\n",
    "    print(f\"Train rows: {len(X_train)} | Test rows: {len(X_test)}\")\n",
    "\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"WARNING: split produced empty set. Skipping model.\")\n",
    "    else:\n",
    "        scaler    = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(np.nan_to_num(X_train))\n",
    "        X_test_s  = scaler.transform(np.nan_to_num(X_test))\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=3,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "\n",
    "        # FIX: use threshold 0.45 instead of 0.5 to improve class 0 recall\n",
    "        y_proba = clf.predict_proba(X_test_s)[:, 1]\n",
    "        y_pred  = (y_proba >= PRED_THRESHOLD).astype(int)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1  = f1_score(y_test, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else float('nan')\n",
    "        cm  = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Cross-validation on full dataset\n",
    "        cv = StratifiedKFold(n_splits=min(5, len(df_model)//10 or 2), shuffle=False)\n",
    "        cv_scores = cross_val_score(clf, scaler.fit_transform(np.nan_to_num(df_model[feat_cols].values)),\n",
    "                                    df_model['target_next_profitable'].values,\n",
    "                                    cv=cv, scoring='roc_auc')\n",
    "\n",
    "        print(\"\\nPREDICTIVE MODEL SUMMARY\")\n",
    "        print(\"------------------------\")\n",
    "        print(f\"Train rows : {len(X_train)} | Test rows: {len(X_test)}\")\n",
    "        print(f\"Accuracy   : {acc:.4f}\")\n",
    "        print(f\"F1 Score   : {f1:.4f}\")\n",
    "        print(f\"AUC        : {auc:.4f}\" if not np.isnan(auc) else \"AUC: n/a\")\n",
    "        print(f\"CV AUC     : {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        print(\"Confusion matrix:\\n\", cm)\n",
    "        print(\"\\nClassification report:\\n\",\n",
    "              classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        feat_imp = (pd.DataFrame({'feature': feat_cols, 'importance': clf.feature_importances_})\n",
    "                    .sort_values('importance', ascending=False))\n",
    "        feat_imp.to_csv(\"outputs/model_feature_importances_partc_final.csv\", index=False)\n",
    "        df_model[['account','date'] + feat_cols + ['target_next_profitable']].to_csv(\n",
    "            \"outputs/model_dataset_partc_final.csv\", index=False)\n",
    "\n",
    "        with open(\"outputs/model_report_partc_final.txt\", \"w\") as fh:\n",
    "            fh.write(f\"Accuracy : {acc:.4f}\\nF1: {f1:.4f}\\nAUC: {auc}\\n\")\n",
    "            fh.write(f\"CV AUC   : {cv_scores.mean():.4f} +/- {cv_scores.std():.4f}\\n\")\n",
    "            fh.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "            fh.write(\"\\n\\nClassification report:\\n\")\n",
    "            fh.write(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.barh(feat_imp['feature'], feat_imp['importance'], color='steelblue')\n",
    "        ax.set_xlabel('Importance'); ax.set_title('Feature Importances')\n",
    "        ax.invert_yaxis(); plt.tight_layout()\n",
    "        plt.savefig(\"outputs/model_feature_importances_partc_final.png\", dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Saved model_feature_importances_partc_final.png\")\n",
    "\n",
    "# -------------------- Summary --------------------\n",
    "pd.DataFrame({\n",
    "    'total_orig_pnl': [tot_orig], 'total_adj_pnl': [tot_adj],\n",
    "    'delta_total': [delta_tot], 'orig_daily_vol': [orig_vol], 'adj_daily_vol': [adj_vol],\n",
    "    'high_leverage_cluster': [high_leverage_cluster],\n",
    "    'consistent_winners_cluster': [consistent_winners_cluster],\n",
    "    'leverage_cap_used': [LEVERAGE_CAP], 'size_increase_pct': [SIZE_INCREASE_PCT],\n",
    "}).to_csv(\"outputs/strategy_simulation_summary_partc_final.csv\", index=False)\n",
    "\n",
    "print(\"\\nAll outputs saved to outputs/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
