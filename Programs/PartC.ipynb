{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee53034-da3e-4986-9a3a-a062d0c5f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in use:\n",
      "  TRADES: outputs/trades_cleaned.csv\n",
      "  DAILY: outputs/daily_metrics.csv\n",
      "  DAILY_ACCT: outputs/daily_account_metrics.csv\n",
      "  ACCT_SEG: outputs/account_segments.csv\n",
      "Detected size_col: size_usd | leverage_col: leverage\n",
      "\n",
      "Cluster means:\n",
      " cluster   avg_leverage  avg_win_rate  avg_trades_per_day\n",
      "       0   24145.189812      0.383982         1532.886139\n",
      "       1   28664.002767      0.586406        11995.333333\n",
      "       2 -445764.113390      0.276284         2641.583333\n",
      "\n",
      "Clusters → high_leverage: 1 | consistent_winners: 0\n",
      "\n",
      "SIMULATION SUMMARY\n",
      "------------------\n",
      "Total orig pnl : 10,296,958.94\n",
      "Total adj  pnl : 9,496,464.51\n",
      "Delta          : -800,494.44\n",
      "Orig daily vol : 2,581,231.98\n",
      "Adj  daily vol : 2,313,607.15\n",
      "\n",
      "Top 5 accounts HURT:\n",
      "                                   account      orig_pnl       adj_pnl         delta\n",
      "0xbaaaf6571ab7d571043ff1e313a9609a10637864  9.401638e+05  1.880380e+05 -7.521258e+05\n",
      "0xbee1707d6b44d4d52bfe19e41f8a828645437aab  8.360806e+05  7.697635e+05 -6.631705e+04\n",
      "0xb1231a4a2dd02f2276fa3c5e2a2f3436e6bfed23  2.143383e+06  2.143383e+06 -2.100132e-07\n",
      "0x8170715b3b381dffb7062c0298972d4727a0a63b -1.676211e+05 -1.676211e+05 -1.841981e-07\n",
      "0x47add9a56df66b524d5e2c1993a43cde53b6ed85  1.033437e+05  1.033437e+05 -1.523731e-07\n",
      "\n",
      "Top 5 accounts HELPED:\n",
      "                                   account      orig_pnl       adj_pnl        delta\n",
      "0x430f09841d65beb3f27765503d0f850b8bce7713 416541.872341 416541.872341 0.000000e+00\n",
      "0x271b280974205ca63b716753467d5a371de622ab -70436.191318 -70436.191318 2.793968e-09\n",
      "0x8477e447846c758f5a675856001ea72298fd9cb5  43917.008976  43917.008976 1.180888e-08\n",
      "0x75f7eeb85dc639d5e99c78f95393aa9a5f1170d4 379095.406711 386600.673086 7.505266e+03\n",
      "0x2c229d22b100a7beb69122eed721cee9b24011dd 168658.004994 179101.150884 1.044315e+04\n",
      "\n",
      "Model dataset size: 102 rows\n",
      "Class distribution: {1: 59, 0: 43}\n",
      "Train rows: 81 | Test rows: 21\n",
      "\n",
      "PREDICTIVE MODEL SUMMARY\n",
      "------------------------\n",
      "Train rows : 81 | Test rows: 21\n",
      "Accuracy   : 0.7619\n",
      "F1 Score   : 0.8148\n",
      "AUC        : 0.8333\n",
      "CV AUC     : 0.7210 ± 0.1081\n",
      "Confusion matrix:\n",
      " [[ 5  4]\n",
      " [ 1 11]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.56      0.67         9\n",
      "           1       0.73      0.92      0.81        12\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.78      0.74      0.74        21\n",
      "weighted avg       0.78      0.76      0.75        21\n",
      "\n",
      "Saved model_feature_importances_partc_final.png\n",
      "\n",
      "All outputs saved to outputs/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PART C (FIXED v2): Strategy Simulation + Predictive Model\n",
    "# Fixes over v1:\n",
    "#   1. Train/test split by ROW INDEX not by date → more training data\n",
    "#   2. Separated high_leverage and consistent_winners clusters\n",
    "#   3. Added 14-day rolling features for better signal\n",
    "#   4. Added SMOTE-style class balancing via class_weight\n",
    "#   5. Lowered prediction threshold (0.45) to improve class 0 recall\n",
    "#   6. Added cross-validation score for reliability check\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------- Config --------------------\n",
    "LEVERAGE_CAP      = 3.0\n",
    "SIZE_INCREASE_PCT = 0.20\n",
    "LEVERAGE_CLIP_PCT = 0.99\n",
    "SCALE_MIN, SCALE_MAX = 0.2, 5.0\n",
    "RANDOM_STATE      = 42\n",
    "PRED_THRESHOLD    = 0.45   # lower threshold to improve class 0 detection\n",
    "# ------------------------------------------------\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "def pick(*paths):\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "TRADES_FILE     = pick(\"outputs/trades_cleaned.csv\",       \"outputs/trades.csv\")\n",
    "DAILY_FILE      = pick(\"outputs/daily_metrics.csv\",        \"outputs/daily.csv\")\n",
    "DAILY_ACCT_FILE = pick(\"outputs/daily_account_metrics.csv\")\n",
    "ACCT_SEG_FILE   = pick(\"outputs/account_segments.csv\",     \"outputs/account_segments_fixed.csv\")\n",
    "\n",
    "missing = [n for n, f in [(\"TRADES\", TRADES_FILE), (\"DAILY\", DAILY_FILE),\n",
    "                            (\"DAILY_ACCT\", DAILY_ACCT_FILE), (\"ACCT_SEG\", ACCT_SEG_FILE)] if not f]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Required output files missing: {missing}. Run Part A & B first.\")\n",
    "\n",
    "print(\"Files in use:\")\n",
    "for label, path in [(\"TRADES\", TRADES_FILE), (\"DAILY\", DAILY_FILE),\n",
    "                     (\"DAILY_ACCT\", DAILY_ACCT_FILE), (\"ACCT_SEG\", ACCT_SEG_FILE)]:\n",
    "    print(f\"  {label}: {path}\")\n",
    "\n",
    "# -------------------- Load --------------------\n",
    "trades        = pd.read_csv(TRADES_FILE,     low_memory=False)\n",
    "daily         = pd.read_csv(DAILY_FILE,      low_memory=False)\n",
    "daily_account = pd.read_csv(DAILY_ACCT_FILE, low_memory=False)\n",
    "acct_seg      = pd.read_csv(ACCT_SEG_FILE,   low_memory=False)\n",
    "\n",
    "# -------------------- Normalize dates --------------------\n",
    "def to_date_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            parsed = pd.to_datetime(df[c], errors='coerce')\n",
    "            if parsed.notna().sum() > 0:\n",
    "                return parsed.dt.normalize()\n",
    "    raise ValueError(\"No parsable date column found among: \" + str(candidates))\n",
    "\n",
    "daily['date']         = to_date_col(daily,         ['date', 'day'])\n",
    "daily_account['date'] = to_date_col(daily_account, ['date'])\n",
    "\n",
    "if 'date' in trades.columns:\n",
    "    trades['date'] = to_date_col(trades, ['date'])\n",
    "elif 'datetime' in trades.columns:\n",
    "    trades['date'] = pd.to_datetime(trades['datetime'], errors='coerce').dt.normalize()\n",
    "else:\n",
    "    raise ValueError(\"trades has neither 'date' nor 'datetime' column.\")\n",
    "\n",
    "# -------------------- Validate --------------------\n",
    "if 'account' not in trades.columns or 'account' not in acct_seg.columns:\n",
    "    raise ValueError(\"Missing 'account' column in trades or account_segments.\")\n",
    "\n",
    "# -------------------- Numeric conversions --------------------\n",
    "trades['closed_pnl'] = pd.to_numeric(\n",
    "    trades.get('closed_pnl', trades.get('pnl', pd.Series(0, index=trades.index))),\n",
    "    errors='coerce').fillna(0.0)\n",
    "\n",
    "size_col = next((c for c in ['size_usd','size','size_tokens','abs_size','notional']\n",
    "                 if c in trades.columns), None)\n",
    "lev_col  = next((c for c in ['leverage','lev','margin','leverage_ratio']\n",
    "                 if c in trades.columns), None)\n",
    "\n",
    "print(\"Detected size_col:\", size_col, \"| leverage_col:\", lev_col)\n",
    "\n",
    "trades['old_leverage'] = pd.to_numeric(trades[lev_col],  errors='coerce') if lev_col  else np.nan\n",
    "trades['old_size']     = pd.to_numeric(trades[size_col], errors='coerce') if size_col else np.nan\n",
    "\n",
    "trades.loc[trades['old_leverage'] == 0, 'old_leverage'] = np.nan\n",
    "trades.loc[trades['old_size']     == 0, 'old_size']     = np.nan\n",
    "\n",
    "if trades['old_leverage'].notna().sum() > 0:\n",
    "    lev_cap_val = trades['old_leverage'].quantile(LEVERAGE_CLIP_PCT)\n",
    "    trades['old_leverage_clipped'] = trades['old_leverage'].clip(lower=1.0, upper=lev_cap_val)\n",
    "else:\n",
    "    trades['old_leverage_clipped'] = 1.0\n",
    "\n",
    "trades['old_size_filled']     = trades['old_size'].fillna(1.0)\n",
    "trades['old_leverage_filled'] = trades['old_leverage_clipped'].fillna(1.0)\n",
    "\n",
    "# -------------------- Merge cluster --------------------\n",
    "acct_seg_small = acct_seg[['account','cluster']].drop_duplicates()\n",
    "trades = trades.merge(acct_seg_small, on='account', how='left')\n",
    "trades['cluster'] = trades['cluster'].fillna(-1).astype(int)\n",
    "\n",
    "# -------------------- Identify clusters --------------------\n",
    "feat_for_cluster = [f for f in ['avg_leverage','avg_win_rate','avg_trades_per_day']\n",
    "                    if f in acct_seg.columns]\n",
    "if not feat_for_cluster:\n",
    "    feat_for_cluster = acct_seg.select_dtypes(include=[np.number]).columns[:2].tolist()\n",
    "\n",
    "cluster_means = acct_seg.groupby('cluster')[feat_for_cluster].mean().reset_index()\n",
    "print(\"\\nCluster means:\")\n",
    "print(cluster_means.to_string(index=False))\n",
    "\n",
    "# High leverage cluster\n",
    "lev_sort_col = 'avg_leverage' if 'avg_leverage' in cluster_means.columns else feat_for_cluster[0]\n",
    "sorted_by_lev = cluster_means.sort_values(lev_sort_col, ascending=False)\n",
    "high_leverage_cluster = int(sorted_by_lev.iloc[0]['cluster'])\n",
    "\n",
    "# FIX: consistent winners must be a DIFFERENT cluster from high_leverage\n",
    "if 'avg_win_rate' in cluster_means.columns:\n",
    "    sorted_by_wr = cluster_means.sort_values('avg_win_rate', ascending=False)\n",
    "    for _, row in sorted_by_wr.iterrows():\n",
    "        if int(row['cluster']) != high_leverage_cluster:\n",
    "            consistent_winners_cluster = int(row['cluster'])\n",
    "            break\n",
    "    else:\n",
    "        consistent_winners_cluster = int(sorted_by_wr.iloc[0]['cluster'])\n",
    "elif 'total_pnl' in acct_seg.columns:\n",
    "    pnl_means = acct_seg.groupby('cluster')['total_pnl'].mean().reset_index().sort_values('total_pnl', ascending=False)\n",
    "    for _, row in pnl_means.iterrows():\n",
    "        if int(row['cluster']) != high_leverage_cluster:\n",
    "            consistent_winners_cluster = int(row['cluster'])\n",
    "            break\n",
    "else:\n",
    "    all_clusters = cluster_means['cluster'].tolist()\n",
    "    consistent_winners_cluster = next((c for c in all_clusters if c != high_leverage_cluster),\n",
    "                                       high_leverage_cluster)\n",
    "\n",
    "print(f\"\\nClusters → high_leverage: {high_leverage_cluster} | consistent_winners: {consistent_winners_cluster}\")\n",
    "\n",
    "# -------------------- Map sentiment --------------------\n",
    "if 'sentiment' in trades.columns:\n",
    "    trades.drop(columns=['sentiment'], inplace=True)\n",
    "\n",
    "sent_map = daily.set_index('date')['sentiment'].to_dict()\n",
    "trades['date_norm'] = pd.to_datetime(trades['date']).dt.normalize()\n",
    "trades['sentiment'] = trades['date_norm'].map(sent_map).ffill().bfill()\n",
    "\n",
    "# -------------------- Apply strategy rules --------------------\n",
    "mask_r1 = ((trades['cluster'] == high_leverage_cluster) &\n",
    "            (trades['sentiment'].str.lower() == 'fear'))\n",
    "trades['new_leverage'] = trades['old_leverage_filled'].copy()\n",
    "trades.loc[mask_r1, 'new_leverage'] = np.minimum(\n",
    "    trades.loc[mask_r1, 'old_leverage_filled'], LEVERAGE_CAP)\n",
    "\n",
    "acct_win = (daily_account.groupby('account')['win_rate']\n",
    "            .mean().reset_index().rename(columns={'win_rate': 'acct_win_rate'}))\n",
    "trades = trades.merge(acct_win, on='account', how='left')\n",
    "\n",
    "mask_r2 = ((trades['cluster'] == consistent_winners_cluster) &\n",
    "            (trades['sentiment'].str.lower() == 'greed') &\n",
    "            (trades['acct_win_rate'] > 0.5))\n",
    "trades['new_size'] = trades['old_size_filled'].copy()\n",
    "trades.loc[mask_r2, 'new_size'] = trades.loc[mask_r2, 'old_size_filled'] * (1 + SIZE_INCREASE_PCT)\n",
    "\n",
    "trades['new_leverage'] = trades['new_leverage'].fillna(trades['old_leverage_filled'])\n",
    "trades['new_size']     = trades['new_size'].fillna(trades['old_size_filled'])\n",
    "\n",
    "# -------------------- Scaling + adjusted PnL --------------------\n",
    "epsilon = 1e-9\n",
    "trades['scale_raw']      = ((trades['new_leverage'] * trades['new_size']) /\n",
    "                             (trades['old_leverage_filled'] * trades['old_size_filled'] + epsilon))\n",
    "trades['scale']          = trades['scale_raw'].clip(lower=SCALE_MIN, upper=SCALE_MAX)\n",
    "trades['closed_pnl_adj'] = trades['closed_pnl'] * trades['scale']\n",
    "\n",
    "# -------------------- Aggregate results --------------------\n",
    "orig_daily = trades.groupby('date_norm')['closed_pnl'].sum().reset_index(name='orig_total_pnl')\n",
    "adj_daily  = trades.groupby('date_norm')['closed_pnl_adj'].sum().reset_index(name='adj_total_pnl')\n",
    "compare_daily = (orig_daily.merge(adj_daily, on='date_norm', how='outer')\n",
    "                 .fillna(0).sort_values('date_norm').reset_index(drop=True))\n",
    "compare_daily['delta'] = compare_daily['adj_total_pnl'] - compare_daily['orig_total_pnl']\n",
    "\n",
    "tot_orig  = compare_daily['orig_total_pnl'].sum()\n",
    "tot_adj   = compare_daily['adj_total_pnl'].sum()\n",
    "delta_tot = tot_adj - tot_orig\n",
    "\n",
    "orig_acct = trades.groupby('account')['closed_pnl'].sum().reset_index(name='orig_pnl')\n",
    "adj_acct  = trades.groupby('account')['closed_pnl_adj'].sum().reset_index(name='adj_pnl')\n",
    "acct_compare = (orig_acct.merge(adj_acct, on='account', how='outer')\n",
    "                .fillna(0).assign(delta=lambda d: d['adj_pnl'] - d['orig_pnl'])\n",
    "                .sort_values('delta'))\n",
    "\n",
    "orig_vol = trades.groupby('date_norm')['closed_pnl'].sum().std()\n",
    "adj_vol  = trades.groupby('date_norm')['closed_pnl_adj'].sum().std()\n",
    "\n",
    "compare_daily.to_csv(\"outputs/strategy_simulation_daily_impact_partc_final.csv\", index=False)\n",
    "trades.to_csv(\"outputs/trades_sim_full_partc_final.csv\", index=False)\n",
    "acct_compare.to_csv(\"outputs/account_impact_partc_final.csv\", index=False)\n",
    "\n",
    "print(\"\\nSIMULATION SUMMARY\")\n",
    "print(\"------------------\")\n",
    "print(f\"Total orig pnl : {tot_orig:,.2f}\")\n",
    "print(f\"Total adj  pnl : {tot_adj:,.2f}\")\n",
    "print(f\"Delta          : {delta_tot:,.2f}\")\n",
    "print(f\"Orig daily vol : {orig_vol:,.2f}\")\n",
    "print(f\"Adj  daily vol : {adj_vol:,.2f}\")\n",
    "print(\"\\nTop 5 accounts HURT:\")\n",
    "print(acct_compare.head(5).to_string(index=False))\n",
    "print(\"\\nTop 5 accounts HELPED:\")\n",
    "print(acct_compare.tail(5).to_string(index=False))\n",
    "\n",
    "# Cumulative PnL plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(compare_daily['date_norm'], compare_daily['orig_total_pnl'].cumsum(), label='Original')\n",
    "ax.plot(compare_daily['date_norm'], compare_daily['adj_total_pnl'].cumsum(),  label='Adjusted')\n",
    "ax.set_title('Cumulative PnL: Original vs Strategy-Adjusted')\n",
    "ax.legend(); plt.tight_layout()\n",
    "plt.savefig(\"outputs/simulation_cumpnl_partc_final.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# Predictive Model — FIXED\n",
    "# ============================================================\n",
    "df = daily_account.copy()\n",
    "df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "df = df.sort_values(['account','date']).reset_index(drop=True)\n",
    "\n",
    "df['next_daily_pnl']         = df.groupby('account')['daily_pnl'].shift(-1)\n",
    "df['target_next_profitable'] = (df['next_daily_pnl'] > 0).astype(int)\n",
    "\n",
    "if 'sentiment' in df.columns:\n",
    "    df.drop(columns=['sentiment'], inplace=True)\n",
    "df = df.merge(daily[['date','sentiment']].drop_duplicates(), on='date', how='left')\n",
    "df['sentiment_num'] = df['sentiment'].map({'Fear': -1, 'Greed': 1}).fillna(0)\n",
    "\n",
    "for col in ['trades_count','avg_leverage','win_rate','avg_trade_size']:\n",
    "    df[col] = pd.to_numeric(df.get(col, 0), errors='coerce').fillna(0)\n",
    "\n",
    "# FIX: added 14-day rolling features for more signal\n",
    "for feat, src in [('rol_trades_7','trades_count'),   ('rol_leverage_7','avg_leverage'),\n",
    "                   ('rol_winrate_7','win_rate'),       ('rol_size_7','avg_trade_size'),\n",
    "                   ('rol_trades_14','trades_count'),   ('rol_leverage_14','avg_leverage'),\n",
    "                   ('rol_winrate_14','win_rate'),      ('rol_pnl_7','daily_pnl')]:\n",
    "    window = 14 if '14' in feat else 7\n",
    "    df[feat] = df.groupby('account')[src].transform(\n",
    "        lambda s: s.rolling(window, min_periods=1).mean())\n",
    "\n",
    "feat_cols = ['sentiment_num','rol_trades_7','rol_leverage_7','rol_winrate_7',\n",
    "             'rol_size_7','rol_trades_14','rol_leverage_14','rol_winrate_14','rol_pnl_7']\n",
    "\n",
    "df_model = df.dropna(subset=feat_cols + ['target_next_profitable']).copy().reset_index(drop=True)\n",
    "print(f\"\\nModel dataset size: {df_model.shape[0]} rows\")\n",
    "print(\"Class distribution:\", df_model['target_next_profitable'].value_counts().to_dict())\n",
    "\n",
    "if df_model.shape[0] < 20:\n",
    "    print(\"WARNING: dataset too small. Skipping model.\")\n",
    "else:\n",
    "    # FIX: split by ROW INDEX (80/20) not by date → guarantees enough training rows\n",
    "    split_idx  = int(len(df_model) * 0.80)\n",
    "    X_train    = df_model.loc[:split_idx-1, feat_cols].values\n",
    "    y_train    = df_model.loc[:split_idx-1, 'target_next_profitable'].values\n",
    "    X_test     = df_model.loc[split_idx:,   feat_cols].values\n",
    "    y_test     = df_model.loc[split_idx:,   'target_next_profitable'].values\n",
    "\n",
    "    print(f\"Train rows: {len(X_train)} | Test rows: {len(X_test)}\")\n",
    "\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"WARNING: split produced empty set. Skipping model.\")\n",
    "    else:\n",
    "        scaler    = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(np.nan_to_num(X_train))\n",
    "        X_test_s  = scaler.transform(np.nan_to_num(X_test))\n",
    "\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=3,\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "\n",
    "        # FIX: use threshold 0.45 instead of 0.5 to improve class 0 recall\n",
    "        y_proba = clf.predict_proba(X_test_s)[:, 1]\n",
    "        y_pred  = (y_proba >= PRED_THRESHOLD).astype(int)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1  = f1_score(y_test, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_test, y_proba) if len(np.unique(y_test)) > 1 else float('nan')\n",
    "        cm  = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Cross-validation on full dataset\n",
    "        cv = StratifiedKFold(n_splits=min(5, len(df_model)//10 or 2), shuffle=False)\n",
    "        cv_scores = cross_val_score(clf, scaler.fit_transform(np.nan_to_num(df_model[feat_cols].values)),\n",
    "                                    df_model['target_next_profitable'].values,\n",
    "                                    cv=cv, scoring='roc_auc')\n",
    "\n",
    "        print(\"\\nPREDICTIVE MODEL SUMMARY\")\n",
    "        print(\"------------------------\")\n",
    "        print(f\"Train rows : {len(X_train)} | Test rows: {len(X_test)}\")\n",
    "        print(f\"Accuracy   : {acc:.4f}\")\n",
    "        print(f\"F1 Score   : {f1:.4f}\")\n",
    "        print(f\"AUC        : {auc:.4f}\" if not np.isnan(auc) else \"AUC: n/a\")\n",
    "        print(f\"CV AUC     : {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        print(\"Confusion matrix:\\n\", cm)\n",
    "        print(\"\\nClassification report:\\n\",\n",
    "              classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        feat_imp = (pd.DataFrame({'feature': feat_cols, 'importance': clf.feature_importances_})\n",
    "                    .sort_values('importance', ascending=False))\n",
    "        feat_imp.to_csv(\"outputs/model_feature_importances_partc_final.csv\", index=False)\n",
    "        df_model[['account','date'] + feat_cols + ['target_next_profitable']].to_csv(\n",
    "            \"outputs/model_dataset_partc_final.csv\", index=False)\n",
    "\n",
    "        with open(\"outputs/model_report_partc_final.txt\", \"w\") as fh:\n",
    "            fh.write(f\"Accuracy : {acc:.4f}\\nF1: {f1:.4f}\\nAUC: {auc}\\n\")\n",
    "            fh.write(f\"CV AUC   : {cv_scores.mean():.4f} +/- {cv_scores.std():.4f}\\n\")\n",
    "            fh.write(\"Confusion matrix:\\n\" + np.array2string(cm))\n",
    "            fh.write(\"\\n\\nClassification report:\\n\")\n",
    "            fh.write(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax.barh(feat_imp['feature'], feat_imp['importance'], color='steelblue')\n",
    "        ax.set_xlabel('Importance'); ax.set_title('Feature Importances')\n",
    "        ax.invert_yaxis(); plt.tight_layout()\n",
    "        plt.savefig(\"outputs/model_feature_importances_partc_final.png\", dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Saved model_feature_importances_partc_final.png\")\n",
    "\n",
    "# -------------------- Summary --------------------\n",
    "pd.DataFrame({\n",
    "    'total_orig_pnl': [tot_orig], 'total_adj_pnl': [tot_adj],\n",
    "    'delta_total': [delta_tot], 'orig_daily_vol': [orig_vol], 'adj_daily_vol': [adj_vol],\n",
    "    'high_leverage_cluster': [high_leverage_cluster],\n",
    "    'consistent_winners_cluster': [consistent_winners_cluster],\n",
    "    'leverage_cap_used': [LEVERAGE_CAP], 'size_increase_pct': [SIZE_INCREASE_PCT],\n",
    "}).to_csv(\"outputs/strategy_simulation_summary_partc_final.csv\", index=False)\n",
    "\n",
    "print(\"\\nAll outputs saved to outputs/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
