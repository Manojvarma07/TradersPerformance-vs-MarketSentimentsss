{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd9d84c-d33b-4038-80e7-e9d099b2db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: sentiment rows = 2644 , trades rows = 211224\n",
      "\n",
      "-- Sentiment (raw) --\n",
      "shape: (2644, 4)\n",
      "columns: ['timestamp', 'value', 'classification', 'date']\n",
      "missing (top 10):\n",
      "timestamp         0\n",
      "value             0\n",
      "classification    0\n",
      "date              0\n",
      "duplicate rows: 0\n",
      "    timestamp  value classification        date\n",
      "0  1517463000     30           Fear  2018-02-01\n",
      "1  1517549400     15   Extreme Fear  2018-02-02\n",
      "2  1517635800     40           Fear  2018-02-03\n",
      "\n",
      "-- Trades (raw) --\n",
      "shape: (211224, 16)\n",
      "columns: ['Account', 'Coin', 'Execution Price', 'Size Tokens', 'Size USD', 'Side', 'Timestamp IST', 'Start Position', 'Direction', 'Closed PnL', 'Transaction Hash', 'Order ID', 'Crossed', 'Fee', 'Trade ID', 'Timestamp']\n",
      "missing (top 10):\n",
      "Account            0\n",
      "Coin               0\n",
      "Execution Price    0\n",
      "Size Tokens        0\n",
      "Size USD           0\n",
      "Side               0\n",
      "Timestamp IST      0\n",
      "Start Position     0\n",
      "Direction          0\n",
      "Closed PnL         0\n",
      "duplicate rows: 0\n",
      "                                      Account  Coin  Execution Price  Size Tokens  Size USD Side     Timestamp IST  Start Position Direction  Closed PnL                                                    Transaction Hash     Order ID  Crossed       Fee      Trade ID     Timestamp\n",
      "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9769       986.87   7872.16  BUY  02-12-2024 22:50        0.000000       Buy         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac0f37caef8a734502ec49  52017706630     True  0.345404  8.950000e+14  1.730000e+12\n",
      "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9800        16.00    127.68  BUY  02-12-2024 22:50      986.524596       Buy         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac0f37caef8a734502ec49  52017706630     True  0.005600  4.430000e+14  1.730000e+12\n",
      "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9855       144.09   1150.63  BUY  02-12-2024 22:50     1002.518996       Buy         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac0f37caef8a734502ec49  52017706630     True  0.050431  6.600000e+14  1.730000e+12\n",
      "\n",
      "Normalized sentiment cols: ['timestamp', 'value', 'classification', 'date']\n",
      "Normalized trades cols: ['account', 'coin', 'execution_price', 'size_tokens', 'size_usd', 'side', 'timestamp_ist', 'start_position', 'direction', 'closed_pnl', 'transaction_hash', 'order_id', 'crossed', 'fee', 'trade_id', 'timestamp']\n",
      "\n",
      "Sentiment normalized: rows = 2644 | date range: 2018-02-01 -> 2025-05-02\n",
      "\n",
      "Detected trade columns:\n",
      "  account: account\n",
      "  timestamp: timestamp\n",
      "  closed_pnl: closed_pnl\n",
      "  size_usd: size_usd\n",
      "  size_tokens: size_tokens\n",
      "  any_size: size_usd\n",
      "  side: side\n",
      "  leverage: start_position\n",
      "\n",
      "Timestamp parsing method: epoch_unit_ms\n",
      "Parsed non-null: 211224 of 211224\n",
      "Sample datetimes: [Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20'), Timestamp('2024-10-27 03:33:20')]\n",
      "Range: 2023-03-28 10:40:00 -> 2025-06-15 15:06:40\n",
      "Dropped 0 rows (invalid datetime). Remaining: 211224\n",
      "Trades date range: 2023-03-28 -> 2025-06-15\n",
      "\n",
      "Canonical columns present: ['account', 'closed_pnl', 'size_usd', 'size_tokens', 'datetime', 'date', 'side', 'leverage']\n",
      "\n",
      "--- Part A Summary ---\n",
      "Trades:          rows,cols: (211224, 20)\n",
      "Daily_account:   rows,cols: (102, 12)\n",
      "Daily (platform):rows,cols: (7, 8)\n",
      "\n",
      "Missing values (trades) top 10:\n",
      "account      0\n",
      "coin         0\n",
      "is_win       0\n",
      "date         0\n",
      "datetime     0\n",
      "timestamp    0\n",
      "trade_id     0\n",
      "fee          0\n",
      "crossed      0\n",
      "order_id     0\n",
      "\n",
      "Missing values (daily_account) top 10:\n",
      "date              0\n",
      "account           0\n",
      "daily_pnl         0\n",
      "trades_count      0\n",
      "win_count         0\n",
      "avg_trade_size    0\n",
      "avg_leverage      0\n",
      "long_count        0\n",
      "short_count       0\n",
      "win_rate          0\n",
      "\n",
      "Duplicates: trades = 0\n",
      "Duplicates: daily_account = 0\n",
      "\n",
      "Leverage summary (trades):\n",
      "count    2.112240e+05\n",
      "mean    -2.994625e+04\n",
      "std      6.738074e+05\n",
      "min     -1.433463e+07\n",
      "25%     -3.762311e+02\n",
      "50%      8.472793e+01\n",
      "75%      9.337278e+03\n",
      "max      3.050948e+07\n",
      "\n",
      "Top 5 accounts by total_pnl:\n",
      "                                   account    total_pnl  total_trades  avg_win_rate\n",
      "0xb1231a4a2dd02f2276fa3c5e2a2f3436e6bfed23 2.143383e+06         14733      0.306417\n",
      "0x083384f897ee0f19899168e3b1bec365f52a9012 1.600230e+06          3818      0.215758\n",
      "0xbaaaf6571ab7d571043ff1e313a9609a10637864 9.401638e+05         21192      0.733766\n",
      "0x513b8629fe877bb581bf244e326a047b249c4ff1 8.404226e+05         12236      0.272598\n",
      "0xbee1707d6b44d4d52bfe19e41f8a828645437aab 8.360806e+05         40184      0.439046\n",
      "\n",
      "Saved to outputs/: daily_account_metrics.csv, daily_metrics.csv, trades_cleaned.csv\n",
      "\n",
      "✅ PART A COMPLETE — Share your Part B code when ready.\n"
     ]
    }
   ],
   "source": [
    "# PART : Data Preparation\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SENTIMENT_PATH = \"fear_greed_index.csv\"\n",
    "TRADER_PATH    = \"historical_data.csv\"\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# ------- Load files -------\n",
    "if not os.path.exists(SENTIMENT_PATH):\n",
    "    raise FileNotFoundError(f\"Sentiment file not found at: {SENTIMENT_PATH}\")\n",
    "if not os.path.exists(TRADER_PATH):\n",
    "    raise FileNotFoundError(f\"Trades file not found at: {TRADER_PATH}\")\n",
    "\n",
    "sent   = pd.read_csv(SENTIMENT_PATH)\n",
    "trades = pd.read_csv(TRADER_PATH, low_memory=False)\n",
    "\n",
    "print(\"Loaded: sentiment rows =\", len(sent), \", trades rows =\", len(trades))\n",
    "\n",
    "# ------- Document shapes / missing / duplicates -------\n",
    "def doc_df(df, name, n_show=3):\n",
    "    print(f\"\\n-- {name} --\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"columns:\", df.columns.tolist())\n",
    "    print(\"missing (top 10):\")\n",
    "    print(df.isnull().sum().sort_values(ascending=False).head(10).to_string())\n",
    "    print(\"duplicate rows:\", df.duplicated().sum())\n",
    "    print(df.head(n_show).to_string())   # FIX: replaced display() with print()\n",
    "\n",
    "doc_df(sent,   \"Sentiment (raw)\")\n",
    "doc_df(trades, \"Trades (raw)\")\n",
    "\n",
    "# ------- Normalize column names -------\n",
    "sent.columns   = sent.columns.str.strip().str.lower().str.replace(r'\\s+', '_', regex=True)\n",
    "trades.columns = (trades.columns\n",
    "                  .str.strip()\n",
    "                  .str.replace('\\xa0', ' ', regex=False)\n",
    "                  .str.strip()\n",
    "                  .str.replace(r'\\s+', '_', regex=True)\n",
    "                  .str.lower())\n",
    "\n",
    "print(\"\\nNormalized sentiment cols:\", list(sent.columns))\n",
    "print(\"Normalized trades cols:\",    list(trades.columns))\n",
    "\n",
    "# ------- Detect sentiment columns -------\n",
    "sent_date_col  = next((c for c in sent.columns if \"date\"  in c), None)\n",
    "sent_label_col = next((c for c in sent.columns\n",
    "                       if any(k in c for k in [\"class\",\"sent\",\"classification\"])), None)\n",
    "if sent_date_col is None or sent_label_col is None:\n",
    "    raise ValueError(\"Couldn't detect sentiment date/class columns. Found: \" + \", \".join(sent.columns))\n",
    "\n",
    "sent['date']      = pd.to_datetime(sent[sent_date_col], errors='coerce').dt.date\n",
    "sent['sentiment'] = sent[sent_label_col].astype(str).str.strip().str.capitalize()\n",
    "sent = sent[['date', 'sentiment']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "print(\"\\nSentiment normalized: rows =\", len(sent),\n",
    "      \"| date range:\", sent['date'].min(), \"->\", sent['date'].max())\n",
    "\n",
    "# ------- Detect trade columns -------\n",
    "def pick(cols):\n",
    "    for c in cols:\n",
    "        if c in trades.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "acct_col        = pick(['account','acct','user','client'])\n",
    "timestamp_col   = pick(['timestamp','timestamp_ist','timestamp_utc','time','ts','datetime'])\n",
    "closed_pnl_col  = next((c for c in trades.columns if 'closed' in c and 'pnl' in c), None) \\\n",
    "                  or pick(['closed_pnl','realized_pnl','pnl','profit'])\n",
    "size_usd_col    = pick(['size_usd','sizeusd','size_usd.'])\n",
    "size_tokens_col = pick(['size_tokens','size_token'])\n",
    "size_col_any    = size_usd_col or size_tokens_col or pick(['size','qty','quantity'])\n",
    "side_col        = pick(['side','direction','trade_side'])\n",
    "leverage_col    = pick(['leverage','lev','leverage_ratio','start_position','margin'])\n",
    "\n",
    "print(\"\\nDetected trade columns:\")\n",
    "for k, v in [(\"account\", acct_col), (\"timestamp\", timestamp_col),\n",
    "              (\"closed_pnl\", closed_pnl_col), (\"size_usd\", size_usd_col),\n",
    "              (\"size_tokens\", size_tokens_col), (\"any_size\", size_col_any),\n",
    "              (\"side\", side_col), (\"leverage\", leverage_col)]:\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "required_missing = [r for r in [acct_col, timestamp_col, closed_pnl_col] if r is None]\n",
    "if required_missing:\n",
    "    raise ValueError(f\"Missing required columns: {required_missing}. Available: {list(trades.columns)}\")\n",
    "\n",
    "# ------- Convert numeric fields safely -------\n",
    "trades[closed_pnl_col] = pd.to_numeric(trades[closed_pnl_col], errors='coerce')\n",
    "if size_col_any:\n",
    "    trades[size_col_any] = pd.to_numeric(trades[size_col_any], errors='coerce')\n",
    "if leverage_col and leverage_col in trades.columns:\n",
    "    trades[leverage_col] = pd.to_numeric(trades[leverage_col], errors='coerce')\n",
    "\n",
    "# ------- Robust timestamp parsing -------\n",
    "def try_parse_epoch_unit(series):\n",
    "    \"\"\"Try units ns/us/ms/s; return (parsed_series, unit) or (None, None).\"\"\"\n",
    "    snum = pd.to_numeric(series, errors='coerce').dropna()\n",
    "    if len(snum) == 0:\n",
    "        return None, None\n",
    "    sample = snum.sample(min(len(snum), 200), random_state=1)\n",
    "    best_unit, best_score = None, -1\n",
    "    for u in ['ns', 'us', 'ms', 's']:\n",
    "        try:\n",
    "            # FIX: cast to plain int64 numpy array to avoid overflow with Int64 nullable\n",
    "            dt_sample = pd.to_datetime(sample.values.astype('int64'), unit=u, errors='coerce')\n",
    "            score = pd.Series(dt_sample).dt.year.between(2009, 2035).sum()\n",
    "            if score > best_score:\n",
    "                best_unit, best_score = u, score\n",
    "        except Exception:\n",
    "            continue\n",
    "    if best_score <= 0:\n",
    "        return None, None\n",
    "    try:\n",
    "        parsed = pd.to_datetime(\n",
    "            pd.to_numeric(series, errors='coerce').values.astype('float64').astype('int64'),\n",
    "            unit=best_unit, errors='coerce'\n",
    "        )\n",
    "        return pd.Series(parsed, index=series.index), best_unit\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def clean_timestamp_strings(s):\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r'\\s+[A-Z]{2,4}$', '', regex=True)  # strip trailing TZ abbreviations\n",
    "    s = s.str.replace(',', ' ', regex=False)\n",
    "    return s\n",
    "\n",
    "# Primary: numeric epoch detection\n",
    "parsed_dt, unit_used = try_parse_epoch_unit(trades[timestamp_col])\n",
    "if parsed_dt is None or parsed_dt.isna().all():\n",
    "    cleaned   = clean_timestamp_strings(trades[timestamp_col])\n",
    "    parsed_dt = pd.to_datetime(cleaned, errors='coerce', utc=False)\n",
    "    method    = \"string_clean_parse\"\n",
    "else:\n",
    "    method = f\"epoch_unit_{unit_used}\"\n",
    "\n",
    "# Fallback: try alternate timestamp columns\n",
    "if parsed_dt.isna().sum() >= len(parsed_dt) * 0.99:\n",
    "    for alt in ['timestamp_ist', 'timestamp_utc', 'time', 'datetime']:\n",
    "        if alt in trades.columns and alt != timestamp_col:\n",
    "            parsed_alt, alt_unit = try_parse_epoch_unit(trades[alt])\n",
    "            if parsed_alt is not None and parsed_alt.notna().sum() > 0:\n",
    "                parsed_dt, method = parsed_alt, f\"alt_epoch_{alt}_{alt_unit}\"\n",
    "                break\n",
    "            parsed_alt = pd.to_datetime(\n",
    "                clean_timestamp_strings(trades[alt]), errors='coerce', utc=False)\n",
    "            if parsed_alt.notna().sum() > 0:\n",
    "                parsed_dt, method = parsed_alt, f\"alt_string_{alt}\"\n",
    "                break\n",
    "\n",
    "trades['datetime'] = parsed_dt\n",
    "print(f\"\\nTimestamp parsing method: {method}\")\n",
    "print(\"Parsed non-null:\", trades['datetime'].notna().sum(), \"of\", len(trades))\n",
    "if trades['datetime'].notna().sum() > 0:\n",
    "    print(\"Sample datetimes:\", trades['datetime'].head(5).tolist())\n",
    "    print(\"Range:\", trades['datetime'].min(), \"->\", trades['datetime'].max())\n",
    "\n",
    "# Fix: re-parse if >40% of dates are 1970 (epoch unit mismatch)\n",
    "if trades['datetime'].notna().sum() > 0:\n",
    "    n_1970   = (trades['datetime'].dt.year == 1970).sum()\n",
    "    frac_1970 = n_1970 / max(1, trades['datetime'].notna().sum())\n",
    "    if frac_1970 > 0.4:\n",
    "        print(f\"  [WARN] {n_1970} rows are year 1970 ({frac_1970:.1%}); re-parsing with best numeric unit...\")\n",
    "        snum = pd.to_numeric(trades[timestamp_col], errors='coerce').dropna()\n",
    "        if len(snum) > 0:\n",
    "            best_unit, best_valid = None, -1\n",
    "            for u in ['ns', 'us', 'ms', 's']:\n",
    "                try:\n",
    "                    dt_try = pd.to_datetime(snum.values.astype('int64'), unit=u, errors='coerce')\n",
    "                    valid  = pd.Series(dt_try).dt.year.between(2009, 2035).sum()\n",
    "                    if valid > best_valid:\n",
    "                        best_unit, best_valid = u, valid\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if best_unit:\n",
    "                trades['datetime'] = pd.to_datetime(\n",
    "                    pd.to_numeric(trades[timestamp_col], errors='coerce')\n",
    "                      .values.astype('float64').astype('int64'),\n",
    "                    unit=best_unit, errors='coerce'\n",
    "                )\n",
    "                print(\"  Reparsed unit:\", best_unit, \"| valid count:\", best_valid)\n",
    "                print(\"  New range:\", trades['datetime'].min(), \"->\", trades['datetime'].max())\n",
    "\n",
    "trades['date']  = trades['datetime'].dt.date\n",
    "before_drop     = len(trades)\n",
    "trades          = trades.dropna(subset=['date']).reset_index(drop=True)\n",
    "print(f\"Dropped {before_drop - len(trades)} rows (invalid datetime). Remaining: {len(trades)}\")\n",
    "print(\"Trades date range:\", trades['date'].min(), \"->\", trades['date'].max())\n",
    "\n",
    "# ------- Rename to canonical column names -------\n",
    "rename_map = {}\n",
    "if acct_col        and acct_col        != 'account':     rename_map[acct_col]        = 'account'\n",
    "if closed_pnl_col  and closed_pnl_col  != 'closed_pnl':  rename_map[closed_pnl_col]  = 'closed_pnl'\n",
    "if size_usd_col    and size_usd_col    != 'size_usd':    rename_map[size_usd_col]    = 'size_usd'\n",
    "if size_tokens_col and size_tokens_col != 'size_tokens': rename_map[size_tokens_col] = 'size_tokens'\n",
    "if size_col_any    and size_col_any not in ['size_usd','size_tokens','size']:\n",
    "    rename_map[size_col_any] = 'size'\n",
    "if side_col        and side_col        != 'side':        rename_map[side_col]        = 'side'\n",
    "if leverage_col    and leverage_col in trades.columns and leverage_col != 'leverage':\n",
    "    rename_map[leverage_col] = 'leverage'\n",
    "\n",
    "trades.rename(columns=rename_map, inplace=True)\n",
    "canonical = [c for c in ['account','closed_pnl','size_usd','size_tokens','size',\n",
    "                          'datetime','date','side','leverage'] if c in trades.columns]\n",
    "print(\"\\nCanonical columns present:\", canonical)\n",
    "\n",
    "# ------- Indicators -------\n",
    "trades['is_win'] = trades['closed_pnl'] > 0\n",
    "if   'size_usd'    in trades.columns: trades['abs_size'] = trades['size_usd'].abs()\n",
    "elif 'size_tokens' in trades.columns: trades['abs_size'] = trades['size_tokens'].abs()\n",
    "elif 'size'        in trades.columns: trades['abs_size'] = trades['size'].abs()\n",
    "else:                                  trades['abs_size'] = np.nan\n",
    "\n",
    "# ------- Daily per-account aggregation -------\n",
    "agg_dict = {\n",
    "    'daily_pnl':       ('closed_pnl', 'sum'),\n",
    "    'trades_count':    ('closed_pnl', 'count'),\n",
    "    'win_count':       ('is_win',     'sum'),\n",
    "    'avg_trade_size':  ('abs_size',   'mean'),\n",
    "}\n",
    "if 'leverage' in trades.columns:\n",
    "    agg_dict['avg_leverage'] = ('leverage', 'mean')\n",
    "\n",
    "daily_account = trades.groupby(['date', 'account']).agg(**agg_dict).reset_index()\n",
    "\n",
    "# Long / short counts\n",
    "if 'side' in trades.columns:\n",
    "    def count_side(s, positives):\n",
    "        return s.astype(str).str.lower().isin(positives).sum()\n",
    "\n",
    "    longs  = (trades.groupby(['date','account'])['side']\n",
    "              .apply(lambda s: count_side(s, {'buy','long'}))    # FIX: no include_groups needed\n",
    "              .reset_index(name='long_count'))\n",
    "    shorts = (trades.groupby(['date','account'])['side']\n",
    "              .apply(lambda s: count_side(s, {'sell','short'}))\n",
    "              .reset_index(name='short_count'))\n",
    "    daily_account = daily_account.merge(longs,  on=['date','account'], how='left')\n",
    "    daily_account = daily_account.merge(shorts, on=['date','account'], how='left')\n",
    "else:\n",
    "    daily_account['long_count']  = np.nan\n",
    "    daily_account['short_count'] = np.nan\n",
    "\n",
    "daily_account['win_rate']         = daily_account['win_count']  / daily_account['trades_count']\n",
    "daily_account['long_short_ratio'] = daily_account['long_count'] / (daily_account['short_count'] + 1e-9)\n",
    "\n",
    "# Intra-day drawdown proxy\n",
    "def day_drawdown(sub):\n",
    "    sub  = sub.sort_values('datetime')\n",
    "    csum = sub['closed_pnl'].cumsum()\n",
    "    return csum.min() if not csum.empty else np.nan\n",
    "\n",
    "# FIX: use include_groups=False to suppress FutureWarning\n",
    "dd = (trades.groupby(['date','account'])\n",
    "      .apply(day_drawdown, include_groups=False)     # pandas ≥ 2.2 compat\n",
    "      .reset_index(name='daily_min_cum_pnl'))\n",
    "daily_account = daily_account.merge(dd, on=['date','account'], how='left')\n",
    "\n",
    "# ------- Platform / day aggregation -------\n",
    "platform_agg = {\n",
    "    'total_pnl':      ('daily_pnl',        'sum'),\n",
    "    'avg_win_rate':   ('win_rate',          'mean'),\n",
    "    'avg_trade_size': ('avg_trade_size',    'mean'),\n",
    "    'total_trades':   ('trades_count',      'sum'),\n",
    "    'avg_long_short': ('long_short_ratio',  'mean'),\n",
    "}\n",
    "if 'avg_leverage' in daily_account.columns:\n",
    "    platform_agg['avg_leverage'] = ('avg_leverage', 'mean')\n",
    "\n",
    "daily = daily_account.groupby('date').agg(**platform_agg).reset_index()\n",
    "daily = daily.merge(sent, on='date', how='left')\n",
    "\n",
    "# ------- Print Part A Summary -------\n",
    "print(\"\\n--- Part A Summary ---\")\n",
    "print(\"Trades:          rows,cols:\", trades.shape)\n",
    "print(\"Daily_account:   rows,cols:\", daily_account.shape)\n",
    "print(\"Daily (platform):rows,cols:\", daily.shape)\n",
    "\n",
    "print(\"\\nMissing values (trades) top 10:\")\n",
    "print(trades.isnull().sum().sort_values(ascending=False).head(10).to_string())\n",
    "\n",
    "print(\"\\nMissing values (daily_account) top 10:\")\n",
    "print(daily_account.isnull().sum().sort_values(ascending=False).head(10).to_string())\n",
    "\n",
    "print(\"\\nDuplicates: trades =\",        trades.duplicated().sum())\n",
    "print(\"Duplicates: daily_account =\",  daily_account.duplicated().sum())\n",
    "\n",
    "if 'leverage' in trades.columns:\n",
    "    print(\"\\nLeverage summary (trades):\")\n",
    "    print(trades['leverage'].describe().to_string())\n",
    "\n",
    "acct_summary = (daily_account.groupby('account')\n",
    "                .agg(total_pnl=('daily_pnl','sum'),\n",
    "                     total_trades=('trades_count','sum'),\n",
    "                     avg_win_rate=('win_rate','mean'))\n",
    "                .reset_index())\n",
    "print(\"\\nTop 5 accounts by total_pnl:\")\n",
    "print(acct_summary.sort_values('total_pnl', ascending=False).head(5).to_string(index=False))\n",
    "\n",
    "# ------- Save outputs -------\n",
    "daily_account.to_csv(\"outputs/daily_account_metrics.csv\", index=False)\n",
    "daily.to_csv(\"outputs/daily_metrics.csv\",         index=False)\n",
    "trades.to_csv(\"outputs/trades_cleaned.csv\",        index=False)\n",
    "\n",
    "print(\"\\nSaved to outputs/: daily_account_metrics.csv, daily_metrics.csv, trades_cleaned.csv\")\n",
    "print(\"\\n✅ PART A COMPLETE — Share your Part B code when ready.\")\n",
    "# ============================================================\n",
    "# END PART A (FIXED)\n",
    "# ============================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
